# This file was auto-generated by Fern from our API Definition.

from ..core.pydantic_utilities import UniversalBaseModel
import typing
import pydantic
from .language import Language
from .model_parameters import ModelParameters
from .citation_parameters import CitationParameters
from ..core.pydantic_utilities import IS_PYDANTIC_V2


class GenerationParameters(UniversalBaseModel):
    """
    The parameters to control generation.
    """

    prompt_name: typing.Optional[str] = pydantic.Field(default=None)
    """
    The prompt to use to feed the query results and other context to the model.
    
    A prompt is an object with a bundle of properties that specifies:
    
    - The `prompt_text` that is rendered then sent to the LLM.
    - The LLM used.
    - `model_parameter`s such as temperature.
    
    All of these properties except the model can be overriden by setting them in this
    object. Even when a `prompt_text` is set, the `prompt_name` is used to set
    the model used.
    
    If `prompt_name` is not set the Vectara platform will use the default model and
    prompt.
    """

    max_used_search_results: typing.Optional[int] = pydantic.Field(default=None)
    """
    The maximum number of search results to be available to the prompt.
    """

    prompt_text: typing.Optional[str] = pydantic.Field(default=None)
    """
    Vectara manages both system and user roles and prompts for the generative
    LLM out of the box by default. However, Scale customers can override the
    prompt_text via this variable. The prompt_text is in the form of an
    Apache Velocity template. For more details on how to configure the
    prompt_text, see the long-form documentation at
    https://docs.vectara.com/docs/prompts/vectara-prompt-engine.
    See https://vectara.com/pricing/ for more details on becoming a Scale customer.
    """

    max_response_characters: typing.Optional[int] = pydantic.Field(default=None)
    """
    Controls the length of the generated output.
    This is a rough estimate and not a hard limit: the end output can be longer or shorter
    than this value. This is generally implemented by including the `max_response_characters` in the
    prompt, and the LLM's instruction following capability dictates how closely the generated output
    is limited.
    
    So, this value This is currently a Scale-only feature.
    See https://vectara.com/pricing/ for more details on becoming a Scale customer.
    """

    response_language: typing.Optional[Language] = None
    model_parameters: typing.Optional[ModelParameters] = pydantic.Field(default=None)
    """
    The parameters for the model. These are currently a Scale-only feature.
    See https://vectara.com/pricing/ for more details on becoming a Scale customer.
    WARNING: This is an experimental feature, and breakable at any point with virtually no
    notice. It is meant for experimentation to converge on optimal parameters that can then
    be set in the prompt definitions.
    """

    citations: typing.Optional[CitationParameters] = None
    enable_factual_consistency_score: typing.Optional[bool] = pydantic.Field(default=None)
    """
    Enable returning the factual consistency score with query results.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow
