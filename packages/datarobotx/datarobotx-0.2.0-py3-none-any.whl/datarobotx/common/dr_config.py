#
# Copyright 2023 DataRobot, Inc. and its affiliates.
#
# All rights reserved.
#
# DataRobot, Inc.
#
# This is proprietary source code of DataRobot, Inc. and its
# affiliates.
#
# Released under the terms of DataRobot Tool and Utility Agreement.
# mypy: disable-error-code="assignment,dict-item"
"""
Autogenerated Configuration Classes:

DRConfig
DataConfig
FeaturesAutoTSConfig
FeaturesConfig
FeaturesSAFERConfig
FeaturesTSFeatureSettingConfig
FeaturesTSPeriodicityConfig
MetadataConfig
ModelingAutoMLConfig
ModelingAutoTSConfig
ModelingBiasFairnessConfig
ModelingConfig
ModelingModeConfig
PartitioningConfig
PartitioningDTBacktestConfig
PartitioningDateTimeConfig
PartitioningGroupConfig
PartitioningUserConfig
TargetAggregationConfig
TargetAutoMLConfig
TargetAutoTSConfig
TargetConfig

"""
# pylint: disable=too-many-lines
from typing import Any, Dict, List, Optional, Union

from datarobotx.common.utils import DrxConfig


class DataConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Row and column selection configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    smart_downsampled : bool
        Whether to use smart downsampling to throw away excess rows of the
        majority class. Only applicable to classification and zero-boosted
        regression projects.
    majority_downsampling_rate : float
        The percentage between 0 and 100 of the majority rows that should be kept.
        Must be specified only if using smart downsampling. If not specified, a
        default will be selected based on the dataset distribution. The chosen
        rate may not cause the majority class to become smaller than the minority
        class.
    external_predictions : list of str
        List of external prediction columns from the dataset.
    featurelist_id : str
        The ID of a featurelist to use for autopilot.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "smart_downsampled",
            "majority_downsampling_rate",
            "external_predictions",
            "featurelist_id",
        ],
        "post_autopilots": ["featurelist_id"],
    }

    def __init__(
        self,
        smart_downsampled: Optional[bool] = None,
        majority_downsampling_rate: Optional[float] = None,
        external_predictions: Optional[List[str]] = None,
        featurelist_id: Optional[str] = None,
    ) -> None:
        self.smart_downsampled = smart_downsampled
        self.majority_downsampling_rate = majority_downsampling_rate
        self.external_predictions = external_predictions
        self.featurelist_id = featurelist_id

    @property
    def smart_downsampled(self) -> bool:
        """
        Whether to use smart downsampling to throw away excess rows of the
        majority class. Only applicable to classification and zero-boosted
        regression projects.

        Notes
        -----
        smart_downsampled : bool
        """
        return self._smart_downsampled

    @smart_downsampled.setter
    def smart_downsampled(self, value: bool) -> None:
        self._smart_downsampled = value

    @property
    def majority_downsampling_rate(self) -> float:
        """
        The percentage between 0 and 100 of the majority rows that should be
        kept. Must be specified only if using smart downsampling. If not
        specified, a default will be selected based on the dataset
        distribution. The chosen rate may not cause the majority class to
        become smaller than the minority class.

        Notes
        -----
        majority_downsampling_rate : float
        """
        return self._majority_downsampling_rate

    @majority_downsampling_rate.setter
    def majority_downsampling_rate(self, value: float) -> None:
        self._majority_downsampling_rate = value

    @property
    def external_predictions(self) -> List[str]:
        """
        List of external prediction columns from the dataset.

        Notes
        -----
        external_predictions : list of str
        """
        return self._external_predictions

    @external_predictions.setter
    def external_predictions(self, value: List[str]) -> None:
        self._external_predictions = value

    @property
    def featurelist_id(self) -> str:
        """
        The ID of a featurelist to use for autopilot.

        Notes
        -----
        featurelist_id : str
        """
        return self._featurelist_id

    @featurelist_id.setter
    def featurelist_id(self, value: str) -> None:
        self._featurelist_id = value


class TargetAggregationConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Target aggregation configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    max_unaggregated_class_values : int
        The maximum number of unique labels before aggregation kicks in. Should be
        at least len(excludedFromAggregation) + 1 for multiclass and at least
        len(excludedFromAggregation) for multilabel.
    min_class_support : int
        Minimum number of instances necessary for each target value in the
        dataset. All values with fewer instances than this value will be
        aggregated
    aggregation_class_name : str
        The name of the class that will be assigned to all rows with aggregated
        classes. Should not match any excluded_from_aggregation or we will have 2
        classes with the same name and no way to distinguish between them. This
        option is only available formulticlass projects. By default
        'DR_RARE_TARGET_VALUES' is used.
    excluded_from_aggregation : list of str
        List of target values that should be guaranteed to kept as is, regardless
        of other settings.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {}

    def __init__(
        self,
        max_unaggregated_class_values: Optional[int] = None,
        min_class_support: Optional[int] = None,
        aggregation_class_name: Optional[str] = None,
        excluded_from_aggregation: Optional[List[str]] = None,
    ) -> None:
        self.max_unaggregated_class_values = max_unaggregated_class_values
        self.min_class_support = min_class_support
        self.aggregation_class_name = aggregation_class_name
        self.excluded_from_aggregation = excluded_from_aggregation

    @property
    def max_unaggregated_class_values(self) -> int:
        """
        The maximum number of unique labels before aggregation kicks in.
        Should be at least len(excludedFromAggregation) + 1 for multiclass and
        at least len(excludedFromAggregation) for multilabel.

        Notes
        -----
        max_unaggregated_class_values : int
        """
        return self._max_unaggregated_class_values

    @max_unaggregated_class_values.setter
    def max_unaggregated_class_values(self, value: int) -> None:
        self._max_unaggregated_class_values = value

    @property
    def min_class_support(self) -> int:
        """
        Minimum number of instances necessary for each target value in the
        dataset. All values with fewer instances than this value will be
        aggregated.

        Notes
        -----
        min_class_support : int
        """
        return self._min_class_support

    @min_class_support.setter
    def min_class_support(self, value: int) -> None:
        self._min_class_support = value

    @property
    def aggregation_class_name(self) -> str:
        """
        The name of the class that will be assigned to all rows with
        aggregated classes. Should not match any excluded_from_aggregation or
        we will have 2 classes with the same name and no way to distinguish
        between them. This option is only available formulticlass projects. By
        default 'DR_RARE_TARGET_VALUES' is used.

        Notes
        -----
        aggregation_class_name : str
        """
        return self._aggregation_class_name

    @aggregation_class_name.setter
    def aggregation_class_name(self, value: str) -> None:
        self._aggregation_class_name = value

    @property
    def excluded_from_aggregation(self) -> List[str]:
        """
        List of target values that should be guaranteed to kept as is,
        regardless of other settings.

        Notes
        -----
        excluded_from_aggregation : list of str
        """
        return self._excluded_from_aggregation

    @excluded_from_aggregation.setter
    def excluded_from_aggregation(self, value: List[str]) -> None:
        self._excluded_from_aggregation = value


class TargetAutoTSConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Time series target(s) configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    forecast_window_start : int
        For time series projects only. How many timeUnits of the
        datetimePartitionColumn into the future relative to the forecast point the
        forecast window should start.
    forecast_window_end : int
        For time series projects only. How many timeUnits of the
        datetimePartitionColumn into the future relative to the forecast point the
        forecast window should end.
    multiseries_id_columns : list of str
        May be used only with time series projects. An array of the column names
        identifying the series to which each row of the dataset belongs. Currently
        only one multiseries ID column is supported. See the multiseries section
        of the time series documentation for more context.
    windows_basis_unit : {'MILLISECOND', 'SECOND', 'MINUTE', 'HOUR', 'DAY', 'WEEK', 'MONTH', 'QUARTER', 'YEAR', 'ROW'}
        For time series projects only. Indicates which unit is basis for feature
        derivation window and forecast window. Valid options are detected time
        unit or `ROW`. If omitted, the default value is detected time unit.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "forecast_window_start",
            "forecast_window_end",
            "multiseries_id_columns",
            "windows_basis_unit",
        ],
        "post_multiseries_properties": ["multiseries_id_columns"],
    }

    def __init__(
        self,
        forecast_window_start: Optional[int] = None,
        forecast_window_end: Optional[int] = None,
        multiseries_id_columns: Optional[List[str]] = None,
        windows_basis_unit: Optional[str] = None,
    ) -> None:
        self.forecast_window_start = forecast_window_start
        self.forecast_window_end = forecast_window_end
        self.multiseries_id_columns = multiseries_id_columns
        self.windows_basis_unit = windows_basis_unit

    @property
    def forecast_window_start(self) -> int:
        """
        For time series projects only. How many timeUnits of the
        datetimePartitionColumn into the future relative to the forecast point
        the forecast window should start.

        Notes
        -----
        forecast_window_start : int
        """
        return self._forecast_window_start

    @forecast_window_start.setter
    def forecast_window_start(self, value: int) -> None:
        self._forecast_window_start = value

    @property
    def forecast_window_end(self) -> int:
        """
        For time series projects only. How many timeUnits of the
        datetimePartitionColumn into the future relative to the forecast point
        the forecast window should end.

        Notes
        -----
        forecast_window_end : int
        """
        return self._forecast_window_end

    @forecast_window_end.setter
    def forecast_window_end(self, value: int) -> None:
        self._forecast_window_end = value

    @property
    def multiseries_id_columns(self) -> List[str]:
        """
        May be used only with time series projects. An array of the column
        names identifying the series to which each row of the dataset belongs.
        Currently only one multiseries ID column is supported. See the
        multiseries section of the time series documentation for more context.

        Notes
        -----
        multiseries_id_columns : list of str
        """
        return self._multiseries_id_columns

    @multiseries_id_columns.setter
    def multiseries_id_columns(self, value: List[str]) -> None:
        self._multiseries_id_columns = value

    @property
    def windows_basis_unit(self) -> str:
        """
        For time series projects only. Indicates which unit is basis for
        feature derivation window and forecast window. Valid options are
        detected time unit or `ROW`. If omitted, the default value is detected
        time unit.

        Notes
        -----
        windows_basis_unit : {'MILLISECOND', 'SECOND', 'MINUTE', 'HOUR', 'DAY', 'WEEK', 'MONTH', 'QUARTER', 'YEAR',
            'ROW'}
        """
        return self._windows_basis_unit

    @windows_basis_unit.setter
    def windows_basis_unit(self, value: str) -> None:
        self._windows_basis_unit = value


class FeaturesTSPeriodicityConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Time series periodicity configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    time_steps : int
        The number of time steps.
    time_unit : {'MILLISECOND', 'SECOND', 'MINUTE', 'HOUR', 'DAY', 'WEEK', 'MONTH', 'QUARTER', 'YEAR', 'ROW'}
        The time unit or `ROW` if windowsBasisUnit is `ROW`

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {}

    def __init__(
        self,
        time_steps: Optional[int] = None,
        time_unit: Optional[str] = None,
    ) -> None:
        self.time_steps = time_steps
        self.time_unit = time_unit

    @property
    def time_steps(self) -> int:
        """
        The number of time steps.

        Notes
        -----
        time_steps : int
        """
        return self._time_steps

    @time_steps.setter
    def time_steps(self, value: int) -> None:
        self._time_steps = value

    @property
    def time_unit(self) -> str:
        """
        The time unit or `ROW` if windowsBasisUnit is `ROW`.

        Notes
        -----
        time_unit : {'MILLISECOND', 'SECOND', 'MINUTE', 'HOUR', 'DAY', 'WEEK', 'MONTH', 'QUARTER', 'YEAR', 'ROW'}
        """
        return self._time_unit

    @time_unit.setter
    def time_unit(self, value: str) -> None:
        self._time_unit = value


class FeaturesTSFeatureSettingConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Time series feature settings.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    feature_name : str
        The name of the feature being specified.
    a_priori : bool
        Renamed to `knownInAdvance`.
    known_in_advance : bool
        For time series projects only. Sets whether the feature is known in
        advance, i.e., values for future dates are known at prediction time. If
        not specified, the feature uses the value from the
        `defaultToKnownInAdvance` flag.
    do_not_derive : bool
        For time series projects only. Sets whether the feature is do-not-derive,
        i.e., is excluded from feature derivation. If not specified, the feature
        uses the value from the `defaultToDoNotDerive` flag.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {}

    def __init__(
        self,
        feature_name: Optional[str] = None,
        a_priori: Optional[bool] = None,
        known_in_advance: Optional[bool] = None,
        do_not_derive: Optional[bool] = None,
    ) -> None:
        self.feature_name = feature_name
        self.a_priori = a_priori
        self.known_in_advance = known_in_advance
        self.do_not_derive = do_not_derive

    @property
    def feature_name(self) -> str:
        """
        The name of the feature being specified.

        Notes
        -----
        feature_name : str
        """
        return self._feature_name

    @feature_name.setter
    def feature_name(self, value: str) -> None:
        self._feature_name = value

    @property
    def a_priori(self) -> bool:
        """
        Renamed to `knownInAdvance`.

        Notes
        -----
        a_priori : bool
        """
        return self._a_priori

    @a_priori.setter
    def a_priori(self, value: bool) -> None:
        self._a_priori = value

    @property
    def known_in_advance(self) -> bool:
        """
        For time series projects only. Sets whether the feature is known in
        advance, i.e., values for future dates are known at prediction time.
        If not specified, the feature uses the value from the
        `defaultToKnownInAdvance` flag.

        Notes
        -----
        known_in_advance : bool
        """
        return self._known_in_advance

    @known_in_advance.setter
    def known_in_advance(self, value: bool) -> None:
        self._known_in_advance = value

    @property
    def do_not_derive(self) -> bool:
        """
        For time series projects only. Sets whether the feature is do-not-
        derive, i.e., is excluded from feature derivation. If not specified,
        the feature uses the value from the `defaultToDoNotDerive` flag.

        Notes
        -----
        do_not_derive : bool
        """
        return self._do_not_derive

    @do_not_derive.setter
    def do_not_derive(self, value: bool) -> None:
        self._do_not_derive = value


class FeaturesSAFERConfig(DrxConfig):  # type: ignore[type-arg]
    """
    SAFER configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    relationships_configuration_id : str
        Relationships configuration id to be used for Feature Discovery projects.
    feature_engineering_prediction_point : str
        The date column to be used as prediction point for time-based feature
        engineering.
    autopilot_with_feature_discovery : bool
        If true, autopilot will run on a feature list that includes features found
        via search for interactions.
    date_removal : bool
        If true, enable creating additional feature lists without dates (does not
        apply to time-aware projects).
    feature_discovery_supervised_feature_reduction : bool
        Run supervised feature reduction for feature discovery projects.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "relationships_configuration_id",
            "feature_engineering_prediction_point",
            "autopilot_with_feature_discovery",
            "date_removal",
            "feature_discovery_supervised_feature_reduction",
        ],
    }

    def __init__(
        self,
        relationships_configuration_id: Optional[str] = None,
        feature_engineering_prediction_point: Optional[str] = None,
        autopilot_with_feature_discovery: Optional[bool] = None,
        date_removal: Optional[bool] = None,
        feature_discovery_supervised_feature_reduction: Optional[bool] = None,
    ) -> None:
        self.relationships_configuration_id = relationships_configuration_id
        self.feature_engineering_prediction_point = feature_engineering_prediction_point
        self.autopilot_with_feature_discovery = autopilot_with_feature_discovery
        self.date_removal = date_removal
        self.feature_discovery_supervised_feature_reduction = (
            feature_discovery_supervised_feature_reduction
        )

    @property
    def relationships_configuration_id(self) -> str:
        """
        Relationships configuration id to be used for Feature Discovery
        projects.

        Notes
        -----
        relationships_configuration_id : str
        """
        return self._relationships_configuration_id

    @relationships_configuration_id.setter
    def relationships_configuration_id(self, value: str) -> None:
        self._relationships_configuration_id = value

    @property
    def feature_engineering_prediction_point(self) -> str:
        """
        The date column to be used as prediction point for time-based feature
        engineering.

        Notes
        -----
        feature_engineering_prediction_point : str
        """
        return self._feature_engineering_prediction_point

    @feature_engineering_prediction_point.setter
    def feature_engineering_prediction_point(self, value: str) -> None:
        self._feature_engineering_prediction_point = value

    @property
    def autopilot_with_feature_discovery(self) -> bool:
        """
        If true, autopilot will run on a feature list that includes features
        found via search for interactions.

        Notes
        -----
        autopilot_with_feature_discovery : bool
        """
        return self._autopilot_with_feature_discovery

    @autopilot_with_feature_discovery.setter
    def autopilot_with_feature_discovery(self, value: bool) -> None:
        self._autopilot_with_feature_discovery = value

    @property
    def date_removal(self) -> bool:
        """
        If true, enable creating additional feature lists without dates (does
        not apply to time-aware projects).

        Notes
        -----
        date_removal : bool
        """
        return self._date_removal

    @date_removal.setter
    def date_removal(self, value: bool) -> None:
        self._date_removal = value

    @property
    def feature_discovery_supervised_feature_reduction(self) -> bool:
        """
        Run supervised feature reduction for feature discovery projects.

        Notes
        -----
        feature_discovery_supervised_feature_reduction : bool
        """
        return self._feature_discovery_supervised_feature_reduction

    @feature_discovery_supervised_feature_reduction.setter
    def feature_discovery_supervised_feature_reduction(self, value: bool) -> None:
        self._feature_discovery_supervised_feature_reduction = value


class PartitioningUserConfig(DrxConfig):  # type: ignore[type-arg]
    """
    User partitioning configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    user_partition_col : str
        The name of the column containing the partition assignments.
    training_level : str or int or float
        The value of the partition column indicating a row is part of the training
        set.
    validation_level : str or int or float
        The value of the partition column indicating a row is part of the
        validation set.
    holdout_level : str or int or float
        The value of the partition column indicating a row is part of the holdout
        set. This level is optional - if not specified or if provided as ``null``,
        then no holdout will be used in the project. However, the column must have
        exactly 2 values in order for this option to be valid
    cv_holdout_level : str or int or float
        The value of the partition column indicating a row is part of the holdout
        set. This level is optional - if not specified or if provided as ``null``,
        then no holdout will be used in the project. The rest of the levels
        indicate which cross validation fold each row should fall into.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "user_partition_col",
            "training_level",
            "validation_level",
            "holdout_level",
            "cv_holdout_level",
        ],
    }

    def __init__(
        self,
        user_partition_col: Optional[str] = None,
        training_level: Optional[Union[str, int, float]] = None,
        validation_level: Optional[Union[str, int, float]] = None,
        holdout_level: Optional[Union[str, int, float]] = None,
        cv_holdout_level: Optional[Union[str, int, float]] = None,
    ) -> None:
        self.user_partition_col = user_partition_col
        self.training_level = training_level
        self.validation_level = validation_level
        self.holdout_level = holdout_level
        self.cv_holdout_level = cv_holdout_level

    @property
    def user_partition_col(self) -> str:
        """
        The name of the column containing the partition assignments.

        Notes
        -----
        user_partition_col : str
        """
        return self._user_partition_col

    @user_partition_col.setter
    def user_partition_col(self, value: str) -> None:
        self._user_partition_col = value

    @property
    def training_level(self) -> Union[str, int, float]:
        """
        The value of the partition column indicating a row is part of the
        training set.

        Notes
        -----
        training_level : str or int or float
        """
        return self._training_level

    @training_level.setter
    def training_level(self, value: Union[str, int, float]) -> None:
        self._training_level = value

    @property
    def validation_level(self) -> Union[str, int, float]:
        """
        The value of the partition column indicating a row is part of the
        validation set.

        Notes
        -----
        validation_level : str or int or float
        """
        return self._validation_level

    @validation_level.setter
    def validation_level(self, value: Union[str, int, float]) -> None:
        self._validation_level = value

    @property
    def holdout_level(self) -> Union[str, int, float]:
        """
        The value of the partition column indicating a row is part of the
        holdout set. This level is optional - if not specified or if provided
        as ``null``, then no holdout will be used in the project. However, the
        column must have exactly 2 values in order for this option to be valid.

        Notes
        -----
        holdout_level : str or int or float
        """
        return self._holdout_level

    @holdout_level.setter
    def holdout_level(self, value: Union[str, int, float]) -> None:
        self._holdout_level = value

    @property
    def cv_holdout_level(self) -> Union[str, int, float]:
        """
        The value of the partition column indicating a row is part of the
        holdout set. This level is optional - if not specified or if provided
        as ``null``, then no holdout will be used in the project. The rest of
        the levels indicate which cross validation fold each row should fall
        into.

        Notes
        -----
        cv_holdout_level : str or int or float
        """
        return self._cv_holdout_level

    @cv_holdout_level.setter
    def cv_holdout_level(self, value: Union[str, int, float]) -> None:
        self._cv_holdout_level = value


class PartitioningGroupConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Group partitioning configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    partition_key_cols : list of str
        An array containing a single string - the name of the group partition
        column

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": ["partition_key_cols"],
    }

    def __init__(
        self,
        partition_key_cols: Optional[List[str]] = None,
    ) -> None:
        self.partition_key_cols = partition_key_cols

    @property
    def partition_key_cols(self) -> List[str]:
        """
        An array containing a single string - the name of the group partition
        column.

        Notes
        -----
        partition_key_cols : list of str
        """
        return self._partition_key_cols

    @partition_key_cols.setter
    def partition_key_cols(self, value: List[str]) -> None:
        self._partition_key_cols = value


class PartitioningDTBacktestConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Date-time backtest configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    index : int
        The index from zero of the backtest specified by this object.
    validation_start_date : str
        A datetime string representing the start date of the validation data for
        this backtest.
    gap_duration : str
        A duration string representing the duration of the gap between the
        training and the validation data for this backtest.
    validation_duration : str
        A duration string representing the duration of the validation data for
        this backtest.
    validation_end_date : str
        A datetime string representing the end date of the validation data for
        this backtest.
    primary_training_start_date : str
        A datetime string representing the start date of the primary training data
        for this backtest.
    primary_training_end_date : str
        A datetime string representing the end date of the primary training data
        for this backtest.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {}

    def __init__(
        self,
        index: Optional[int] = None,
        validation_start_date: Optional[str] = None,
        gap_duration: Optional[str] = None,
        validation_duration: Optional[str] = None,
        validation_end_date: Optional[str] = None,
        primary_training_start_date: Optional[str] = None,
        primary_training_end_date: Optional[str] = None,
    ) -> None:
        self.index = index
        self.validation_start_date = validation_start_date
        self.gap_duration = gap_duration
        self.validation_duration = validation_duration
        self.validation_end_date = validation_end_date
        self.primary_training_start_date = primary_training_start_date
        self.primary_training_end_date = primary_training_end_date

    @property
    def index(self) -> int:
        """
        The index from zero of the backtest specified by this object.

        Notes
        -----
        index : int
        """
        return self._index

    @index.setter
    def index(self, value: int) -> None:
        self._index = value

    @property
    def validation_start_date(self) -> str:
        """
        A datetime string representing the start date of the validation data
        for this backtest.

        Notes
        -----
        validation_start_date : str
        """
        return self._validation_start_date

    @validation_start_date.setter
    def validation_start_date(self, value: str) -> None:
        self._validation_start_date = value

    @property
    def gap_duration(self) -> str:
        """
        A duration string representing the duration of the gap between the
        training and the validation data for this backtest.

        Notes
        -----
        gap_duration : str
        """
        return self._gap_duration

    @gap_duration.setter
    def gap_duration(self, value: str) -> None:
        self._gap_duration = value

    @property
    def validation_duration(self) -> str:
        """
        A duration string representing the duration of the validation data for
        this backtest.

        Notes
        -----
        validation_duration : str
        """
        return self._validation_duration

    @validation_duration.setter
    def validation_duration(self, value: str) -> None:
        self._validation_duration = value

    @property
    def validation_end_date(self) -> str:
        """
        A datetime string representing the end date of the validation data for
        this backtest.

        Notes
        -----
        validation_end_date : str
        """
        return self._validation_end_date

    @validation_end_date.setter
    def validation_end_date(self, value: str) -> None:
        self._validation_end_date = value

    @property
    def primary_training_start_date(self) -> str:
        """
        A datetime string representing the start date of the primary training
        data for this backtest.

        Notes
        -----
        primary_training_start_date : str
        """
        return self._primary_training_start_date

    @primary_training_start_date.setter
    def primary_training_start_date(self, value: str) -> None:
        self._primary_training_start_date = value

    @property
    def primary_training_end_date(self) -> str:
        """
        A datetime string representing the end date of the primary training
        data for this backtest.

        Notes
        -----
        primary_training_end_date : str
        """
        return self._primary_training_end_date

    @primary_training_end_date.setter
    def primary_training_end_date(self, value: str) -> None:
        self._primary_training_end_date = value


class ModelingModeConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Autopilot modes and metric(s).

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    metric : str
        The metric to use to select the best models. See
        `/api/v2/projects/(projectId)/features/metrics/` for the metrics that may
        be valid for a potential target. Note that weighted metrics must be used
        with a weights column.
    mode : {'4', 'manual', '2', '0', 'auto', 'comprehensive', '3', 'quick'}
        The autopilot mode to use. Either 'quick', 'auto', 'manual' or
        'comprehensive'
    use_time_series : bool
        A boolean value indicating whether a time series project should be created
        instead of a regular project which uses datetime partitioning.
    unsupervised_mode : bool
        If True, unsupervised project (without target) will be created. ``target``
        cannot be specified if ``unsupervisedMode`` is True.
    unsupervised_type : {'anomaly', 'clustering'}
        The type of unsupervised project. Only valid when `unsupervisedMode` is
        true. If `unsupervisedMode`, defaults to `anomaly`.
    shap_only_mode : bool
        Keep only models that support SHAP values during Autopilot run. Use SHAP-
        based insights wherever possible.
    quantile_level : float
        The quantile level between 0.01 and 0.99 for specifying the Quantile
        metric.
    accuracy_optimized_mb : bool
        Include additional, longer-running models that will be run by the
        autopilot and available to run manually.
    only_include_monotonic_blueprints : bool
        When true, only blueprints that support enforcing montonic constraints
        will be available in the project or selected for autopilot.
    monotonic_increasing_featurelist_id : str
        The ID of the featurelist that defines the set of features with a
        monotonically increasing relationship to the target. If null, no such
        constraints are enforced. When specified, this will set a default for the
        project that can be overridden at model submission time if desired.
    monotonic_decreasing_featurelist_id : str
        The ID of the featurelist that defines the set of features with a
        monotonically decreasing relationship to the target. If null, no such
        constraints are enforced. When specified, this will set a default for the
        project that can be overridden at model submission time if desired.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "metric",
            "mode",
            "use_time_series",
            "unsupervised_mode",
            "unsupervised_type",
            "shap_only_mode",
            "quantile_level",
            "accuracy_optimized_mb",
            "only_include_monotonic_blueprints",
            "monotonic_increasing_featurelist_id",
            "monotonic_decreasing_featurelist_id",
        ],
    }

    def __init__(
        self,
        metric: Optional[str] = None,
        mode: Optional[str] = None,
        use_time_series: Optional[bool] = None,
        unsupervised_mode: Optional[bool] = None,
        unsupervised_type: Optional[str] = None,
        shap_only_mode: Optional[bool] = None,
        quantile_level: Optional[float] = None,
        accuracy_optimized_mb: Optional[bool] = None,
        only_include_monotonic_blueprints: Optional[bool] = None,
        monotonic_increasing_featurelist_id: Optional[str] = None,
        monotonic_decreasing_featurelist_id: Optional[str] = None,
    ) -> None:
        self.metric = metric
        self.mode = mode
        self.use_time_series = use_time_series
        self.unsupervised_mode = unsupervised_mode
        self.unsupervised_type = unsupervised_type
        self.shap_only_mode = shap_only_mode
        self.quantile_level = quantile_level
        self.accuracy_optimized_mb = accuracy_optimized_mb
        self.only_include_monotonic_blueprints = only_include_monotonic_blueprints
        self.monotonic_increasing_featurelist_id = monotonic_increasing_featurelist_id
        self.monotonic_decreasing_featurelist_id = monotonic_decreasing_featurelist_id

    @property
    def metric(self) -> str:
        """
        The metric to use to select the best models. See
        `/api/v2/projects/(projectId)/features/metrics/` for the metrics that
        may be valid for a potential target. Note that weighted metrics must
        be used with a weights column.

        Notes
        -----
        metric : str
        """
        return self._metric

    @metric.setter
    def metric(self, value: str) -> None:
        self._metric = value

    @property
    def mode(self) -> str:
        """
        The autopilot mode to use. Either 'quick', 'auto', 'manual' or
        'comprehensive'.

        Notes
        -----
        mode : {'4', 'manual', '2', '0', 'auto', 'comprehensive', '3', 'quick'}
        """
        return self._mode

    @mode.setter
    def mode(self, value: str) -> None:
        self._mode = value

    @property
    def use_time_series(self) -> bool:
        """
        A boolean value indicating whether a time series project should be
        created instead of a regular project which uses datetime partitioning.

        Notes
        -----
        use_time_series : bool
        """
        return self._use_time_series

    @use_time_series.setter
    def use_time_series(self, value: bool) -> None:
        self._use_time_series = value

    @property
    def unsupervised_mode(self) -> bool:
        """
        If True, unsupervised project (without target) will be created.
        ``target`` cannot be specified if ``unsupervisedMode`` is True.

        Notes
        -----
        unsupervised_mode : bool
        """
        return self._unsupervised_mode

    @unsupervised_mode.setter
    def unsupervised_mode(self, value: bool) -> None:
        self._unsupervised_mode = value

    @property
    def unsupervised_type(self) -> str:
        """
        The type of unsupervised project. Only valid when `unsupervisedMode`
        is true. If `unsupervisedMode`, defaults to `anomaly`.

        Notes
        -----
        unsupervised_type : {'anomaly', 'clustering'}
        """
        return self._unsupervised_type

    @unsupervised_type.setter
    def unsupervised_type(self, value: str) -> None:
        self._unsupervised_type = value

    @property
    def shap_only_mode(self) -> bool:
        """
        Keep only models that support SHAP values during Autopilot run. Use
        SHAP-based insights wherever possible.

        Notes
        -----
        shap_only_mode : bool
        """
        return self._shap_only_mode

    @shap_only_mode.setter
    def shap_only_mode(self, value: bool) -> None:
        self._shap_only_mode = value

    @property
    def quantile_level(self) -> float:
        """
        The quantile level between 0.01 and 0.99 for specifying the Quantile
        metric.

        Notes
        -----
        quantile_level : float
        """
        return self._quantile_level

    @quantile_level.setter
    def quantile_level(self, value: float) -> None:
        self._quantile_level = value

    @property
    def accuracy_optimized_mb(self) -> bool:
        """
        Include additional, longer-running models that will be run by the
        autopilot and available to run manually.

        Notes
        -----
        accuracy_optimized_mb : bool
        """
        return self._accuracy_optimized_mb

    @accuracy_optimized_mb.setter
    def accuracy_optimized_mb(self, value: bool) -> None:
        self._accuracy_optimized_mb = value

    @property
    def only_include_monotonic_blueprints(self) -> bool:
        """
        When true, only blueprints that support enforcing montonic constraints
        will be available in the project or selected for autopilot.

        Notes
        -----
        only_include_monotonic_blueprints : bool
        """
        return self._only_include_monotonic_blueprints

    @only_include_monotonic_blueprints.setter
    def only_include_monotonic_blueprints(self, value: bool) -> None:
        self._only_include_monotonic_blueprints = value

    @property
    def monotonic_increasing_featurelist_id(self) -> str:
        """
        The ID of the featurelist that defines the set of features with a
        monotonically increasing relationship to the target. If null, no such
        constraints are enforced. When specified, this will set a default for
        the project that can be overridden at model submission time if desired.

        Notes
        -----
        monotonic_increasing_featurelist_id : str
        """
        return self._monotonic_increasing_featurelist_id

    @monotonic_increasing_featurelist_id.setter
    def monotonic_increasing_featurelist_id(self, value: str) -> None:
        self._monotonic_increasing_featurelist_id = value

    @property
    def monotonic_decreasing_featurelist_id(self) -> str:
        """
        The ID of the featurelist that defines the set of features with a
        monotonically decreasing relationship to the target. If null, no such
        constraints are enforced. When specified, this will set a default for
        the project that can be overridden at model submission time if desired.

        Notes
        -----
        monotonic_decreasing_featurelist_id : str
        """
        return self._monotonic_decreasing_featurelist_id

    @monotonic_decreasing_featurelist_id.setter
    def monotonic_decreasing_featurelist_id(self, value: str) -> None:
        self._monotonic_decreasing_featurelist_id = value


class ModelingAutoMLConfig(DrxConfig):  # type: ignore[type-arg]
    """
    AutoML additional modeling options.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    prepare_model_for_deployment : bool
        Prepare model for deployment during Autopilot run. The preparation
        includes creating reduced feature list models, retraining best model on
        higher sample size, computing insights and assigning 'RECOMMENDED FOR
        DEPLOYMENT' label.
    consider_blenders_in_recommendation : bool
        Include blenders when selecting a model to prepare for deployment in an
        Autopilot Run. This option is not supported in SHAP-only mode or for
        multilabel projects.
    run_leakage_removed_feature_list : bool
        Run Autopilot on Leakage Removed feature list (if exists).
    sample_step_pct : float
        A float between 0 and 100 indicating the desired percentage of data to
        sample when training models in comprehensive Autopilot. Note: this only
        supported for comprehensive Autopilot and the specified value may be
        lowered in order to be compatible with the project's dataset and partition
        settings.
    blend_best_models : bool
        Blend best models during Autopilot run. This option is not supported in
        SHAP-only mode or for multilabel projects.
    blueprint_threshold : int
        The runtime (in hours) which if exceeded will exclude a model from
        autopilot runs.
    scoring_code_only : bool
        Keep only models that can be converted to scorable java code during
        Autopilot run.
    seed : int
        A seed to use for randomization.
    allowed_pairwise_interaction_groups : list of list of str
        For GAM models - specify groups of columns for which pairwise interactions
        will be allowed. E.g. if set to [['A', 'B', 'C'], ['C', 'D']] then GAM
        models will allow interactions between columns AxB, BxC, AxC, CxD. All
        others (AxD, BxD) will not be considered. If not specified - all possible
        interactions will be considered by model.
    stop_words : list of str
        A list of stop words to be used for text blueprints. Note:
        ``stop_words=True`` must be set in the blueprint preprocessing parameters
        for this list of stop words to actually be used during preprocessing.
    min_secondary_validation_model_count : int
        Compute 'All backtest' scores (datetime models) or cross validation scores
        for the specified number of highest ranking models on the Leaderboard, if
        over the Autopilot default.
    autopilot_cluster_list : list of int
        A list of integers where each value will be used as the number of clusters
        in Autopilot model(s) for unsupervised clustering projects. Cannot be
        specified unless `unsupervisedMode` is true and `unsupervisedType` is set
        to `clustering`.
    rate_top_pct_threshold : float
        The percentage threshold between 0.1 and 50 for specifying the Rate@Top%
        metric.
    incremental_learning_only_mode : bool
        Keep only models that support incremental learning during Autopilot run.
    incremental_learning_on_best_model : bool
        Run incremental learning on the best model during Autopilot run.
    number_of_incremental_learning_iterations_before_best_model_selection: int
        Number of iterations top 5 models complete prior to best model selection.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "prepare_model_for_deployment",
            "consider_blenders_in_recommendation",
            "run_leakage_removed_feature_list",
            "sample_step_pct",
            "blend_best_models",
            "blueprint_threshold",
            "scoring_code_only",
            "seed",
            "allowed_pairwise_interaction_groups",
            "stop_words",
            "min_secondary_validation_model_count",
            "autopilot_cluster_list",
            "rate_top_pct_threshold",
            "incremental_learning_only_mode",
            "incremental_learning_on_best_model",
        ],
        "post_autopilots": [
            "prepare_model_for_deployment",
            "consider_blenders_in_recommendation",
            "run_leakage_removed_feature_list",
            "blend_best_models",
            "scoring_code_only",
            "autopilot_cluster_list",
        ],
    }

    def __init__(
        self,
        prepare_model_for_deployment: Optional[bool] = None,
        consider_blenders_in_recommendation: Optional[bool] = None,
        run_leakage_removed_feature_list: Optional[bool] = None,
        sample_step_pct: Optional[float] = None,
        blend_best_models: Optional[bool] = None,
        blueprint_threshold: Optional[int] = None,
        scoring_code_only: Optional[bool] = None,
        seed: Optional[int] = None,
        allowed_pairwise_interaction_groups: Optional[List[List[str]]] = None,
        stop_words: Optional[List[str]] = None,
        min_secondary_validation_model_count: Optional[int] = None,
        autopilot_cluster_list: Optional[List[int]] = None,
        rate_top_pct_threshold: Optional[float] = None,
        incremental_learning_only_mode: Optional[bool] = None,
        incremental_learning_on_best_model: Optional[bool] = None,
        number_of_incremental_learning_iterations_before_best_model_selection: Optional[int] = None,
    ) -> None:
        self.prepare_model_for_deployment = prepare_model_for_deployment
        self.consider_blenders_in_recommendation = consider_blenders_in_recommendation
        self.run_leakage_removed_feature_list = run_leakage_removed_feature_list
        self.sample_step_pct = sample_step_pct
        self.blend_best_models = blend_best_models
        self.blueprint_threshold = blueprint_threshold
        self.scoring_code_only = scoring_code_only
        self.seed = seed
        self.allowed_pairwise_interaction_groups = allowed_pairwise_interaction_groups
        self.stop_words = stop_words
        self.min_secondary_validation_model_count = min_secondary_validation_model_count
        self.autopilot_cluster_list = autopilot_cluster_list
        self.rate_top_pct_threshold = rate_top_pct_threshold
        self.incremental_learning_only_mode = incremental_learning_only_mode
        self.incremental_learning_on_best_model = incremental_learning_on_best_model
        self.number_of_incremental_learning_iterations_before_best_model_selection = (
            number_of_incremental_learning_iterations_before_best_model_selection
        )

    @property
    def prepare_model_for_deployment(self) -> bool:
        """
        Prepare model for deployment during Autopilot run. The preparation
        includes creating reduced feature list models, retraining best model
        on higher sample size, computing insights and assigning 'RECOMMENDED
        FOR DEPLOYMENT' label.

        Notes
        -----
        prepare_model_for_deployment : bool
        """
        return self._prepare_model_for_deployment

    @prepare_model_for_deployment.setter
    def prepare_model_for_deployment(self, value: bool) -> None:
        self._prepare_model_for_deployment = value

    @property
    def consider_blenders_in_recommendation(self) -> bool:
        """
        Include blenders when selecting a model to prepare for deployment in
        an Autopilot Run. This option is not supported in SHAP-only mode or
        for multilabel projects.

        Notes
        -----
        consider_blenders_in_recommendation : bool
        """
        return self._consider_blenders_in_recommendation

    @consider_blenders_in_recommendation.setter
    def consider_blenders_in_recommendation(self, value: bool) -> None:
        self._consider_blenders_in_recommendation = value

    @property
    def run_leakage_removed_feature_list(self) -> bool:
        """
        Run Autopilot on Leakage Removed feature list (if exists).

        Notes
        -----
        run_leakage_removed_feature_list : bool
        """
        return self._run_leakage_removed_feature_list

    @run_leakage_removed_feature_list.setter
    def run_leakage_removed_feature_list(self, value: bool) -> None:
        self._run_leakage_removed_feature_list = value

    @property
    def sample_step_pct(self) -> float:
        """
        A float between 0 and 100 indicating the desired percentage of data to
        sample when training models in comprehensive Autopilot. Note: this
        only supported for comprehensive Autopilot and the specified value may
        be lowered in order to be compatible with the project's dataset and
        partition settings.

        Notes
        -----
        sample_step_pct : float
        """
        return self._sample_step_pct

    @sample_step_pct.setter
    def sample_step_pct(self, value: float) -> None:
        self._sample_step_pct = value

    @property
    def blend_best_models(self) -> bool:
        """
        Blend best models during Autopilot run. This option is not supported
        in SHAP-only mode or for multilabel projects.

        Notes
        -----
        blend_best_models : bool
        """
        return self._blend_best_models

    @blend_best_models.setter
    def blend_best_models(self, value: bool) -> None:
        self._blend_best_models = value

    @property
    def blueprint_threshold(self) -> int:
        """
        The runtime (in hours) which if exceeded will exclude a model from
        autopilot runs.

        Notes
        -----
        blueprint_threshold : int
        """
        return self._blueprint_threshold

    @blueprint_threshold.setter
    def blueprint_threshold(self, value: int) -> None:
        self._blueprint_threshold = value

    @property
    def scoring_code_only(self) -> bool:
        """
        Keep only models that can be converted to scorable java code during
        Autopilot run.

        Notes
        -----
        scoring_code_only : bool
        """
        return self._scoring_code_only

    @scoring_code_only.setter
    def scoring_code_only(self, value: bool) -> None:
        self._scoring_code_only = value

    @property
    def seed(self) -> int:
        """
        A seed to use for randomization.

        Notes
        -----
        seed : int
        """
        return self._seed

    @seed.setter
    def seed(self, value: int) -> None:
        self._seed = value

    @property
    def allowed_pairwise_interaction_groups(self) -> List[List[str]]:
        """
        For GAM models - specify groups of columns for which pairwise
        interactions will be allowed. E.g. if set to [['A', 'B', 'C'], ['C',
        'D']] then GAM models will allow interactions between columns AxB,
        BxC, AxC, CxD. All others (AxD, BxD) will not be considered. If not
        specified - all possible interactions will be considered by model.

        Notes
        -----
        allowed_pairwise_interaction_groups : list of list of str
        """
        return self._allowed_pairwise_interaction_groups

    @allowed_pairwise_interaction_groups.setter
    def allowed_pairwise_interaction_groups(self, value: List[List[str]]) -> None:
        self._allowed_pairwise_interaction_groups = value

    @property
    def stop_words(self) -> List[str]:
        """
        A list of stop words to be used for text blueprints. Note:
        ``stop_words=True`` must be set in the blueprint preprocessing
        parameters for this list of stop words to actually be used during
        preprocessing.

        Notes
        -----
        stop_words : list of str
        """
        return self._stop_words

    @stop_words.setter
    def stop_words(self, value: List[str]) -> None:
        self._stop_words = value

    @property
    def min_secondary_validation_model_count(self) -> int:
        """
        Compute 'All backtest' scores (datetime models) or cross validation
        scores for the specified number of highest ranking models on the
        Leaderboard, if over the Autopilot default.

        Notes
        -----
        min_secondary_validation_model_count : int
        """
        return self._min_secondary_validation_model_count

    @min_secondary_validation_model_count.setter
    def min_secondary_validation_model_count(self, value: int) -> None:
        self._min_secondary_validation_model_count = value

    @property
    def autopilot_cluster_list(self) -> List[int]:
        """
        A list of integers where each value will be used as the number of
        clusters in Autopilot model(s) for unsupervised clustering projects.
        Cannot be specified unless `unsupervisedMode` is true and
        `unsupervisedType` is set to `clustering`.

        Notes
        -----
        autopilot_cluster_list : list of int
        """
        return self._autopilot_cluster_list

    @autopilot_cluster_list.setter
    def autopilot_cluster_list(self, value: List[int]) -> None:
        self._autopilot_cluster_list = value

    @property
    def rate_top_pct_threshold(self) -> float:
        """
        The percentage threshold between 0.1 and 50 for specifying the
        Rate@Top% metric.

        Notes
        -----
        rate_top_pct_threshold : float
        """
        return self._rate_top_pct_threshold

    @rate_top_pct_threshold.setter
    def rate_top_pct_threshold(self, value: float) -> None:
        self._rate_top_pct_threshold = value

    @property
    def incremental_learning_only_mode(self) -> bool:
        """Keep only models that can do incremental learning"""
        return self._incremental_learning_only_mode

    @incremental_learning_only_mode.setter
    def incremental_learning_only_mode(self, value: bool) -> None:
        self._incremental_learning_only_mode = value

    @property
    def incremental_learning_on_best_model(self) -> bool:
        """Run incremental learning on best model during autopilot"""
        return self._incremental_learning_on_best_model

    @incremental_learning_on_best_model.setter
    def incremental_learning_on_best_model(self, value: bool) -> None:
        self._incremental_learning_on_best_model = value

    @property
    def number_of_incremental_learning_iterations_before_best_model_selection(self) -> int:
        return self._number_of_incremental_learning_iterations_before_best_model_selection

    @number_of_incremental_learning_iterations_before_best_model_selection.setter
    def number_of_incremental_learning_iterations_before_best_model_selection(
        self, value: int
    ) -> None:
        self._number_of_incremental_learning_iterations_before_best_model_selection = value


class ModelingBiasFairnessConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Bias and Fairness modeling options.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    bias_mitigation_feature_name : str
        The name of the protected feature used to mitigate bias on models.
    bias_mitigation_technique : {'preprocessingReweighing', 'postProcessingRejectionOptionBasedClassification'}
        Method applied to perform bias mitigation.
    protected_features : list of str
        A list of project feature to mark as protected for Bias metric calculation
        and Fairness correction. Used and required only if *Bias & Fairness in
        AutoML* feature is enabled.
    fairness_metrics_set : {'proportionalParity', 'equalParity', 'predictionBalance',
            'trueFavorableAndUnfavorableRateParity', 'favorableAndUnfavorablePredictiveValueParity'}
        Metric to use for calculating fairness. Can be one of
        ``proportionalParity``, ``equalParity``, ``predictionBalance``,
        ``trueFavorableAndUnfavorableRateParity`` or
        ``FavorableAndUnfavorablePredictiveValueParity``. Used and required only
        if *Bias & Fairness in AutoML* feature is enabled.
    fairness_threshold : float
        Threshold value of the fairness metric. Can be in a range of ``[0:1]``. If
        the actual metric value is below the threshold, the user will be notified
    include_bias_mitigation_feature_as_predictor_variable : bool
        Specifies whether the mitigation feature will be used as a predictor
        variable (i.e., treated like other categorical features in the input to
        train the modeler), in addition to being used for bias mitigation. If
        false, the mitigation feature will be used only for bias mitigation, and
        not for training the modeler task.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "bias_mitigation_feature_name",
            "bias_mitigation_technique",
            "protected_features",
            "fairness_metrics_set",
            "fairness_threshold",
            "include_bias_mitigation_feature_as_predictor_variable",
        ],
    }

    def __init__(
        self,
        bias_mitigation_feature_name: Optional[str] = None,
        bias_mitigation_technique: Optional[str] = None,
        protected_features: Optional[List[str]] = None,
        fairness_metrics_set: Optional[str] = None,
        fairness_threshold: Optional[float] = None,
        include_bias_mitigation_feature_as_predictor_variable: Optional[bool] = None,
    ) -> None:
        self.bias_mitigation_feature_name = bias_mitigation_feature_name
        self.bias_mitigation_technique = bias_mitigation_technique
        self.protected_features = protected_features
        self.fairness_metrics_set = fairness_metrics_set
        self.fairness_threshold = fairness_threshold
        self.include_bias_mitigation_feature_as_predictor_variable = (
            include_bias_mitigation_feature_as_predictor_variable
        )

    @property
    def bias_mitigation_feature_name(self) -> str:
        """
        The name of the protected feature used to mitigate bias on models.

        Notes
        -----
        bias_mitigation_feature_name : str
        """
        return self._bias_mitigation_feature_name

    @bias_mitigation_feature_name.setter
    def bias_mitigation_feature_name(self, value: str) -> None:
        self._bias_mitigation_feature_name = value

    @property
    def bias_mitigation_technique(self) -> str:
        """
        Method applied to perform bias mitigation.

        Notes
        -----
        bias_mitigation_technique : {'preprocessingReweighing', 'postProcessingRejectionOptionBasedClassification'}
        """
        return self._bias_mitigation_technique

    @bias_mitigation_technique.setter
    def bias_mitigation_technique(self, value: str) -> None:
        self._bias_mitigation_technique = value

    @property
    def protected_features(self) -> List[str]:
        """
        A list of project feature to mark as protected for Bias metric
        calculation and Fairness correction. Used and required only if *Bias &
        Fairness in AutoML* feature is enabled.

        Notes
        -----
        protected_features : list of str
        """
        return self._protected_features

    @protected_features.setter
    def protected_features(self, value: List[str]) -> None:
        self._protected_features = value

    @property
    def fairness_metrics_set(self) -> str:
        """
        Metric to use for calculating fairness. Can be one of
        ``proportionalParity``, ``equalParity``, ``predictionBalance``,
        ``trueFavorableAndUnfavorableRateParity`` or
        ``FavorableAndUnfavorablePredictiveValueParity``. Used and required
        only if *Bias & Fairness in AutoML* feature is enabled.

        Notes
        -----
        fairness_metrics_set : {'proportionalParity', 'equalParity', 'predictionBalance',
            'trueFavorableAndUnfavorableRateParity', 'favorableAndUnfavorablePredictiveValueParity'}
        """
        return self._fairness_metrics_set

    @fairness_metrics_set.setter
    def fairness_metrics_set(self, value: str) -> None:
        self._fairness_metrics_set = value

    @property
    def fairness_threshold(self) -> float:
        """
        Threshold value of the fairness metric. Can be in a range of
        ``[0:1]``. If the actual metric value is below the threshold, the user
        will be notified.

        Notes
        -----
        fairness_threshold : float
        """
        return self._fairness_threshold

    @fairness_threshold.setter
    def fairness_threshold(self, value: float) -> None:
        self._fairness_threshold = value

    @property
    def include_bias_mitigation_feature_as_predictor_variable(self) -> bool:
        """
        Specifies whether the mitigation feature will be used as a predictor
        variable (i.e., treated like other categorical features in the input
        to train the modeler), in addition to being used for bias mitigation.
        If false, the mitigation feature will be used only for bias
        mitigation, and not for training the modeler task.

        Notes
        -----
        include_bias_mitigation_feature_as_predictor_variable : bool
        """
        return self._include_bias_mitigation_feature_as_predictor_variable

    @include_bias_mitigation_feature_as_predictor_variable.setter
    def include_bias_mitigation_feature_as_predictor_variable(self, value: bool) -> None:
        self._include_bias_mitigation_feature_as_predictor_variable = value


class ModelingAutoTSConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Time series modeling options.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    model_splits : int
        Sets the cap on the number of jobs per model used when building models to
        control number of jobs in the queue. Higher number of modelSplits will
        allow for less downsampling leading to the use of more post-processed
        data.
    allow_partial_history_time_series_predictions : bool
        Specifies whether the time series predictions can use partial historical
        data.
    external_time_series_baseline_dataset_id : str
        Catalog version id for external prediction data that can be used as a
        baseline to calculate new metrics.
    external_time_series_baseline_dataset_name : str
        The name of the time series baseline dataset for the project.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "model_splits",
            "allow_partial_history_time_series_predictions",
            "external_time_series_baseline_dataset_id",
            "external_time_series_baseline_dataset_name",
        ],
    }

    def __init__(
        self,
        model_splits: Optional[int] = None,
        allow_partial_history_time_series_predictions: Optional[bool] = None,
        external_time_series_baseline_dataset_id: Optional[str] = None,
        external_time_series_baseline_dataset_name: Optional[str] = None,
    ) -> None:
        self.model_splits = model_splits
        self.allow_partial_history_time_series_predictions = (
            allow_partial_history_time_series_predictions
        )
        self.external_time_series_baseline_dataset_id = external_time_series_baseline_dataset_id
        self.external_time_series_baseline_dataset_name = external_time_series_baseline_dataset_name

    @property
    def model_splits(self) -> int:
        """
        Sets the cap on the number of jobs per model used when building models
        to control number of jobs in the queue. Higher number of modelSplits
        will allow for less downsampling leading to the use of more post-
        processed data.

        Notes
        -----
        model_splits : int
        """
        return self._model_splits

    @model_splits.setter
    def model_splits(self, value: int) -> None:
        self._model_splits = value

    @property
    def allow_partial_history_time_series_predictions(self) -> bool:
        """
        Specifies whether the time series predictions can use partial
        historical data.

        Notes
        -----
        allow_partial_history_time_series_predictions : bool
        """
        return self._allow_partial_history_time_series_predictions

    @allow_partial_history_time_series_predictions.setter
    def allow_partial_history_time_series_predictions(self, value: bool) -> None:
        self._allow_partial_history_time_series_predictions = value

    @property
    def external_time_series_baseline_dataset_id(self) -> str:
        """
        Catalog version id for external prediction data that can be used as a
        baseline to calculate new metrics.

        Notes
        -----
        external_time_series_baseline_dataset_id : str
        """
        return self._external_time_series_baseline_dataset_id

    @external_time_series_baseline_dataset_id.setter
    def external_time_series_baseline_dataset_id(self, value: str) -> None:
        self._external_time_series_baseline_dataset_id = value

    @property
    def external_time_series_baseline_dataset_name(self) -> str:
        """
        The name of the time series baseline dataset for the project.

        Notes
        -----
        external_time_series_baseline_dataset_name : str
        """
        return self._external_time_series_baseline_dataset_name

    @external_time_series_baseline_dataset_name.setter
    def external_time_series_baseline_dataset_name(self, value: str) -> None:
        self._external_time_series_baseline_dataset_name = value


class MetadataConfig(DrxConfig):  # type: ignore[type-arg]
    """
    DataRobot metadata and worker configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    project_name : str
        The new name of the project, if it should be renamed.
    project_description : str
        The new description of the project, if the description should be updated.
    worker_count : int
        The desired new number of workers if the number of workers should be
        changed. Must not exceed the number of workers available to the user. `0`
        is allowed. (New in version v2.14) `-1` requests the maximum number
        available to the user.
    gpu_worker_count : int
        The desired new number of gpu workers if the number of gpu workers should
        be changed. Must not exceed the number of gpu workers available to the
        user. `0` is allowed. `-1` requests the maximum number available to the
        user.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_projects": [
            "project_name",
            "project_description",
            "worker_count",
            "gpu_worker_count",
        ],
        "post_projects": ["project_name"],
    }

    def __init__(
        self,
        project_name: Optional[str] = None,
        project_description: Optional[str] = None,
        worker_count: Optional[int] = None,
        gpu_worker_count: Optional[int] = None,
    ) -> None:
        self.project_name = project_name
        self.project_description = project_description
        self.worker_count = worker_count
        self.gpu_worker_count = gpu_worker_count

    @property
    def project_name(self) -> str:
        """
        The new name of the project, if it should be renamed.

        Notes
        -----
        project_name : str
        """
        return self._project_name

    @project_name.setter
    def project_name(self, value: str) -> None:
        self._project_name = value

    @property
    def project_description(self) -> str:
        """
        The new description of the project, if the description should be
        updated.

        Notes
        -----
        project_description : str
        """
        return self._project_description

    @project_description.setter
    def project_description(self, value: str) -> None:
        self._project_description = value

    @property
    def worker_count(self) -> int:
        """
        The desired new number of workers if the number of workers should be
        changed. Must not exceed the number of workers available to the user.
        `0` is allowed. (New in version v2.14) `-1` requests the maximum
        number available to the user.

        Notes
        -----
        worker_count : int
        """
        return self._worker_count

    @worker_count.setter
    def worker_count(self, value: int) -> None:
        self._worker_count = value

    @property
    def gpu_worker_count(self) -> int:
        """
        The desired new number of gpu workers if the number of gpu workers
        should be changed. Must not exceed the number of gpu workers available
        to the user. `0` is allowed. `-1` requests the maximum number
        available to the user.

        Notes
        -----
        gpu_worker_count : int
        """
        return self._gpu_worker_count

    @gpu_worker_count.setter
    def gpu_worker_count(self, value: int) -> None:
        self._gpu_worker_count = value


class ModelingConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Modeling configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    Mode : dict or ModelingModeConfig
        Autopilot modes and metric(s)
    AutoML : dict or ModelingAutoMLConfig
        AutoML additional modeling options
    AutoTS : dict or ModelingAutoTSConfig
        Time series modeling options
    BiasFairness : dict or ModelingBiasFairnessConfig
        Bias and Fairness modeling options

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {}

    def __init__(
        self,
        Mode: Optional[Union[Dict[str, Any], ModelingModeConfig]] = None,
        AutoML: Optional[Union[Dict[str, Any], ModelingAutoMLConfig]] = None,
        AutoTS: Optional[Union[Dict[str, Any], ModelingAutoTSConfig]] = None,
        BiasFairness: Optional[Union[Dict[str, Any], ModelingBiasFairnessConfig]] = None,
    ) -> None:
        if isinstance(Mode, ModelingModeConfig):
            self.Mode = Mode
        elif isinstance(Mode, dict):
            self.Mode = ModelingModeConfig(**Mode)
        else:
            self.Mode = ModelingModeConfig()
        self.__class__.Mode.__doc__ = self.Mode.__doc__

        if isinstance(AutoML, ModelingAutoMLConfig):
            self.AutoML = AutoML
        elif isinstance(AutoML, dict):
            self.AutoML = ModelingAutoMLConfig(**AutoML)
        else:
            self.AutoML = ModelingAutoMLConfig()
        self.__class__.AutoML.__doc__ = self.AutoML.__doc__

        if isinstance(AutoTS, ModelingAutoTSConfig):
            self.AutoTS = AutoTS
        elif isinstance(AutoTS, dict):
            self.AutoTS = ModelingAutoTSConfig(**AutoTS)
        else:
            self.AutoTS = ModelingAutoTSConfig()
        self.__class__.AutoTS.__doc__ = self.AutoTS.__doc__

        if isinstance(BiasFairness, ModelingBiasFairnessConfig):
            self.BiasFairness = BiasFairness
        elif isinstance(BiasFairness, dict):
            self.BiasFairness = ModelingBiasFairnessConfig(**BiasFairness)
        else:
            self.BiasFairness = ModelingBiasFairnessConfig()
        self.__class__.BiasFairness.__doc__ = self.BiasFairness.__doc__

    @property
    def Mode(self) -> ModelingModeConfig:
        """
        Autopilot modes and metric(s).

        Notes
        -----
        Mode : dict or ModelingModeConfig
        """
        return self._Mode

    @Mode.setter
    def Mode(self, value: ModelingModeConfig) -> None:
        self._Mode = value

    @property
    def AutoML(self) -> ModelingAutoMLConfig:
        """
        AutoML additional modeling options.

        Notes
        -----
        AutoML : dict or ModelingAutoMLConfig
        """
        return self._AutoML

    @AutoML.setter
    def AutoML(self, value: ModelingAutoMLConfig) -> None:
        self._AutoML = value

    @property
    def AutoTS(self) -> ModelingAutoTSConfig:
        """
        Time series modeling options.

        Notes
        -----
        AutoTS : dict or ModelingAutoTSConfig
        """
        return self._AutoTS

    @AutoTS.setter
    def AutoTS(self, value: ModelingAutoTSConfig) -> None:
        self._AutoTS = value

    @property
    def BiasFairness(self) -> ModelingBiasFairnessConfig:
        """
        Bias and Fairness modeling options.

        Notes
        -----
        BiasFairness : dict or ModelingBiasFairnessConfig
        """
        return self._BiasFairness

    @BiasFairness.setter
    def BiasFairness(self, value: ModelingBiasFairnessConfig) -> None:
        self._BiasFairness = value


class TargetAutoMLConfig(DrxConfig):  # type: ignore[type-arg]
    """
    AutoML additional target options.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    positive_class : str or int or float
        A value from the target column to use for the positive class. May only be
        specified for projects doing binary classification.If not specified, a
        positive class is selected automatically.
    weights : str
        The name of a column specifying row weights. The data in this column must
        be pure numeric (e.g. not currency, date, length, etc.) and without
        missing values
    offset : list of str
        An array of strings with names of a columns specifying row offsets.The
        data in this column must be pure numeric (e.g. not currency, date, length,
        etc.) and without missing values
    exposure : str
        The name of a column specifying row exposure.The data in this column must
        be pure numeric (e.g. not currency, date, length, etc.) and without
        missing values
    response_cap : float
        Used to cap the maximum response of a model
    events_count : str
        The name of a column specifying events count. The data in this column must
        be pure numeric and non negative without missing values
    preferable_target_value : str or int or float
        A target value that should be treated as a positive outcome for the
        prediction. For example if we want to check gender discrimination for
        giving a loan and our target named ``is_bad``, then the positive outcome
        for the prediction would be ``No``, which means that the loan is good and
        that's what we treat as a preferable result for the loaner. Used and
        required only if *Bias & Fairness in AutoML* feature is enabled.
    class_mapping_aggregation_settings : TargetAggregationConfig
        Class mapping aggregation settings.
    series_id : str
        The name of a column specifying row series id.
    forecast_distance : str
        The name of a column specifying row forecast distance.
    forecast_offsets : list of str
        An array of strings with names of a columns specifying row offsets.The
        data in this column must be pure numeric (e.g. not currency, date, length,
        etc.).
    chunk_definition_id : string
        Unique definition for chunks needed to run automated incremental learning.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "positive_class",
            "weights",
            "offset",
            "exposure",
            "response_cap",
            "events_count",
            "preferable_target_value",
            "class_mapping_aggregation_settings",
            "series_id",
            "forecast_distance",
            "forecast_offsets",
            "chunk_definition_id",
        ],
    }

    def __init__(
        self,
        positive_class: Optional[Union[str, int, float]] = None,
        weights: Optional[str] = None,
        offset: Optional[List[str]] = None,
        exposure: Optional[str] = None,
        response_cap: Optional[float] = None,
        events_count: Optional[str] = None,
        preferable_target_value: Optional[Union[str, int, float]] = None,
        class_mapping_aggregation_settings: Optional[TargetAggregationConfig] = None,
        series_id: Optional[str] = None,
        forecast_distance: Optional[str] = None,
        forecast_offsets: Optional[List[str]] = None,
        chunk_definition_id: Optional[str] = None,
    ) -> None:
        self.positive_class = positive_class
        self.weights = weights
        self.offset = offset
        self.exposure = exposure
        self.response_cap = response_cap
        self.events_count = events_count
        self.preferable_target_value = preferable_target_value
        self.class_mapping_aggregation_settings = class_mapping_aggregation_settings
        self.series_id = series_id
        self.forecast_distance = forecast_distance
        self.forecast_offsets = forecast_offsets
        self.chunk_definition_id = chunk_definition_id

    @property
    def positive_class(self) -> Union[str, int, float]:
        """
        A value from the target column to use for the positive class. May only
        be specified for projects doing binary classification.If not
        specified, a positive class is selected automatically.

        Notes
        -----
        positive_class : str or int or float
        """
        return self._positive_class

    @positive_class.setter
    def positive_class(self, value: Union[str, int, float]) -> None:
        self._positive_class = value

    @property
    def weights(self) -> str:
        """
        The name of a column specifying row weights. The data in this column
        must be pure numeric (e.g. not currency, date, length, etc.) and
        without missing values.

        Notes
        -----
        weights : str
        """
        return self._weights

    @weights.setter
    def weights(self, value: str) -> None:
        self._weights = value

    @property
    def offset(self) -> List[str]:
        """
        An array of strings with names of a columns specifying row offsets.The
        data in this column must be pure numeric (e.g. not currency, date,
        length, etc.) and without missing values.

        Notes
        -----
        offset : list of str
        """
        return self._offset

    @offset.setter
    def offset(self, value: List[str]) -> None:
        self._offset = value

    @property
    def exposure(self) -> str:
        """
        The name of a column specifying row exposure.The data in this column
        must be pure numeric (e.g. not currency, date, length, etc.) and
        without missing values.

        Notes
        -----
        exposure : str
        """
        return self._exposure

    @exposure.setter
    def exposure(self, value: str) -> None:
        self._exposure = value

    @property
    def response_cap(self) -> float:
        """
        Used to cap the maximum response of a model.

        Notes
        -----
        response_cap : float
        """
        return self._response_cap

    @response_cap.setter
    def response_cap(self, value: float) -> None:
        self._response_cap = value

    @property
    def events_count(self) -> str:
        """
        The name of a column specifying events count. The data in this column
        must be pure numeric and non negative without missing values.

        Notes
        -----
        events_count : str
        """
        return self._events_count

    @events_count.setter
    def events_count(self, value: str) -> None:
        self._events_count = value

    @property
    def preferable_target_value(self) -> Union[str, int, float]:
        """
        A target value that should be treated as a positive outcome for the
        prediction. For example if we want to check gender discrimination for
        giving a loan and our target named ``is_bad``, then the positive
        outcome for the prediction would be ``No``, which means that the loan
        is good and that's what we treat as a preferable result for the
        loaner. Used and required only if *Bias & Fairness in AutoML* feature
        is enabled.

        Notes
        -----
        preferable_target_value : str or int or float
        """
        return self._preferable_target_value

    @preferable_target_value.setter
    def preferable_target_value(self, value: Union[str, int, float]) -> None:
        self._preferable_target_value = value

    @property
    def class_mapping_aggregation_settings(self) -> TargetAggregationConfig:
        """
        Class mapping aggregation settings.

        Notes
        -----
        class_mapping_aggregation_settings : TargetAggregationConfig
        """
        return self._class_mapping_aggregation_settings

    @class_mapping_aggregation_settings.setter
    def class_mapping_aggregation_settings(self, value: TargetAggregationConfig) -> None:
        self._class_mapping_aggregation_settings = value

    @property
    def forecast_offsets(self) -> List[str]:
        """
        An array of strings with names of a columns specifying row offsets.The
        data in this column must be pure numeric (e.g. not currency, date,
        length, etc.).

        Notes
        -----
        forecast_offsets : list of str
        """
        return self._forecast_offsets

    @forecast_offsets.setter
    def forecast_offsets(self, value: List[str]) -> None:
        self._forecast_offsets = value

    @property
    def series_id(self) -> str:
        """
        The name of a column specifying row series id.

        Notes
        -----
        series : str
        """
        return self._series_id

    @series_id.setter
    def series_id(self, value: str) -> None:
        self._series_id = value

    @property
    def forecast_distance(self) -> str:
        """
        The name of a column specifying row forecast distance.

        Notes
        -----
        forecast_distance : str
        """
        return self._forecast_distance

    @forecast_distance.setter
    def forecast_distance(self, value: str) -> None:
        self._forecast_distance = value

    @property
    def chunk_definition_id(self) -> str:
        """
        The Mongo ID that uniquely defines chunk definitions needed for incremental learning.

        Notes
        -----
        chunk_definition_id : str
        """
        return self._chunk_definition_id

    @chunk_definition_id.setter
    def chunk_definition_id(self, value: str) -> None:
        self._chunk_definition_id = value


class FeaturesAutoTSConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Time series featurization configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    feature_derivation_window_start : int
        For time series projects only. How many timeUnits of the
        datetimePartitionColumn into the past relative to the forecast point the
        feature derivation window should begin.
    feature_derivation_window_end : int
        For time series projects only. How many timeUnits of the
        datetimePartitionColumn into the past relative to the forecast point the
        feature derivation window should end.
    differencing_method : {'auto', 'none', 'simple', 'seasonal'}
        For time series projects only. Used to specify which differencing method
        to apply if the data is stationary. For classification problems `simple`
        and `seasonal` are not allowed. Parameter `periodicities` must be
        specified if `seasonal` is chosen. Defaults to `auto`.
    exponentially_weighted_moving_alpha : float
        Discount factor (alpha) used for exponentially weighted moving features
    default_to_known_in_advance : bool
        For time series projects only. Sets whether all features default to being
        treated as known in advance features, which are features that are known
        into the future. Features marked as known in advance must be specified
        into the future when making predictions. The default is false, all
        features are not known in advance. Individual features can be set to a
        value different than the default using the `featureSettings` parameter.
        See the Time Series Overview for more context.
    default_to_do_not_derive : bool
        For time series projects only. Sets whether all features default to being
        treated as do-not-derive features, excluding them from feature derivation.
        Individual features can be set to a value different than the default by
        using the `featureSettings` parameter.
    calendar_id : str
        The ID of the calendar to be used in this project.
    aggregation_type : {'total', 'average'}
        For multiseries projects only. The aggregation type to apply when creating
        cross-series features.
    cross_series_group_by_columns : list of str
        For multiseries projects with cross-series features enabled only. List of
        columns (currently of length 1). Setting that indicates how to further
        split series into related groups. For example, if every series is sales of
        an individual product, the series group-by could be the product category
        with values like "men's clothing", "sports equipment", etc.
    segmentation_task_id : str
        Specifies the SegmentationTask that will be used for dividing the project
        up into multiple segmented projects.
    treat_as_exponential : {'auto', 'never', 'always'}
        For time series projects only. Used to specify whether to treat data as
        exponential trend and apply transformations like log-transform. For
        classification problems `always` is not allowed.
    use_supervised_feature_reduction : bool
        When true, during feature generation DataRobot runs a supervised algorithm
        feature lists using only qualifying features. Setting false can severely
        impact autopilot duration, especially for datasets with many features.
    use_cross_series_features : bool
        Indicating if user wants to use cross-series features.
    periodicities : list of FeaturesTSPeriodicityConfig
        A list of periodicities for time series projects only. For classification
        problems periodicities are not allowed. If this is provided, parameter
        'differencing_method' will default to 'seasonal' if not provided or
        'auto'.
    feature_settings : list of FeaturesTSFeatureSettingConfig
        An array specifying per feature settings. Features can be left
        unspecified.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "feature_derivation_window_start",
            "feature_derivation_window_end",
            "differencing_method",
            "exponentially_weighted_moving_alpha",
            "default_to_known_in_advance",
            "default_to_do_not_derive",
            "calendar_id",
            "aggregation_type",
            "cross_series_group_by_columns",
            "segmentation_task_id",
            "treat_as_exponential",
            "use_supervised_feature_reduction",
            "use_cross_series_features",
            "periodicities",
            "feature_settings",
        ],
    }

    def __init__(
        self,
        feature_derivation_window_start: Optional[int] = None,
        feature_derivation_window_end: Optional[int] = None,
        differencing_method: Optional[str] = None,
        exponentially_weighted_moving_alpha: Optional[float] = None,
        default_to_known_in_advance: Optional[bool] = None,
        default_to_do_not_derive: Optional[bool] = None,
        calendar_id: Optional[str] = None,
        aggregation_type: Optional[str] = None,
        cross_series_group_by_columns: Optional[List[str]] = None,
        segmentation_task_id: Optional[str] = None,
        treat_as_exponential: Optional[str] = None,
        use_supervised_feature_reduction: Optional[bool] = None,
        use_cross_series_features: Optional[bool] = None,
        periodicities: Optional[List[FeaturesTSPeriodicityConfig]] = None,
        feature_settings: Optional[List[FeaturesTSFeatureSettingConfig]] = None,
    ) -> None:
        self.feature_derivation_window_start = feature_derivation_window_start
        self.feature_derivation_window_end = feature_derivation_window_end
        self.differencing_method = differencing_method
        self.exponentially_weighted_moving_alpha = exponentially_weighted_moving_alpha
        self.default_to_known_in_advance = default_to_known_in_advance
        self.default_to_do_not_derive = default_to_do_not_derive
        self.calendar_id = calendar_id
        self.aggregation_type = aggregation_type
        self.cross_series_group_by_columns = cross_series_group_by_columns
        self.segmentation_task_id = segmentation_task_id
        self.treat_as_exponential = treat_as_exponential
        self.use_supervised_feature_reduction = use_supervised_feature_reduction
        self.use_cross_series_features = use_cross_series_features
        self.periodicities = periodicities
        self.feature_settings = feature_settings

    @property
    def feature_derivation_window_start(self) -> int:
        """
        For time series projects only. How many timeUnits of the
        datetimePartitionColumn into the past relative to the forecast point
        the feature derivation window should begin.

        Notes
        -----
        feature_derivation_window_start : int
        """
        return self._feature_derivation_window_start

    @feature_derivation_window_start.setter
    def feature_derivation_window_start(self, value: int) -> None:
        self._feature_derivation_window_start = value

    @property
    def feature_derivation_window_end(self) -> int:
        """
        For time series projects only. How many timeUnits of the
        datetimePartitionColumn into the past relative to the forecast point
        the feature derivation window should end.

        Notes
        -----
        feature_derivation_window_end : int
        """
        return self._feature_derivation_window_end

    @feature_derivation_window_end.setter
    def feature_derivation_window_end(self, value: int) -> None:
        self._feature_derivation_window_end = value

    @property
    def differencing_method(self) -> str:
        """
        For time series projects only. Used to specify which differencing
        method to apply if the data is stationary. For classification problems
        `simple` and `seasonal` are not allowed. Parameter `periodicities`
        must be specified if `seasonal` is chosen. Defaults to `auto`.

        Notes
        -----
        differencing_method : {'auto', 'none', 'simple', 'seasonal'}
        """
        return self._differencing_method

    @differencing_method.setter
    def differencing_method(self, value: str) -> None:
        self._differencing_method = value

    @property
    def exponentially_weighted_moving_alpha(self) -> float:
        """
        Discount factor (alpha) used for exponentially weighted moving
        features.

        Notes
        -----
        exponentially_weighted_moving_alpha : float
        """
        return self._exponentially_weighted_moving_alpha

    @exponentially_weighted_moving_alpha.setter
    def exponentially_weighted_moving_alpha(self, value: float) -> None:
        self._exponentially_weighted_moving_alpha = value

    @property
    def default_to_known_in_advance(self) -> bool:
        """
        For time series projects only. Sets whether all features default to
        being treated as known in advance features, which are features that
        are known into the future. Features marked as known in advance must be
        specified into the future when making predictions. The default is
        false, all features are not known in advance. Individual features can
        be set to a value different than the default using the
        `featureSettings` parameter. See the Time Series Overview for more
        context.

        Notes
        -----
        default_to_known_in_advance : bool
        """
        return self._default_to_known_in_advance

    @default_to_known_in_advance.setter
    def default_to_known_in_advance(self, value: bool) -> None:
        self._default_to_known_in_advance = value

    @property
    def default_to_do_not_derive(self) -> bool:
        """
        For time series projects only. Sets whether all features default to
        being treated as do-not-derive features, excluding them from feature
        derivation. Individual features can be set to a value different than
        the default by using the `featureSettings` parameter.

        Notes
        -----
        default_to_do_not_derive : bool
        """
        return self._default_to_do_not_derive

    @default_to_do_not_derive.setter
    def default_to_do_not_derive(self, value: bool) -> None:
        self._default_to_do_not_derive = value

    @property
    def calendar_id(self) -> str:
        """
        The ID of the calendar to be used in this project.

        Notes
        -----
        calendar_id : str
        """
        return self._calendar_id

    @calendar_id.setter
    def calendar_id(self, value: str) -> None:
        self._calendar_id = value

    @property
    def aggregation_type(self) -> str:
        """
        For multiseries projects only. The aggregation type to apply when
        creating cross-series features.

        Notes
        -----
        aggregation_type : {'total', 'average'}
        """
        return self._aggregation_type

    @aggregation_type.setter
    def aggregation_type(self, value: str) -> None:
        self._aggregation_type = value

    @property
    def cross_series_group_by_columns(self) -> List[str]:
        """
        For multiseries projects with cross-series features enabled only. List
        of columns (currently of length 1). Setting that indicates how to
        further split series into related groups. For example, if every series
        is sales of an individual product, the series group-by could be the
        product category with values like "men's clothing", "sports
        equipment", etc.

        Notes
        -----
        cross_series_group_by_columns : list of str
        """
        return self._cross_series_group_by_columns

    @cross_series_group_by_columns.setter
    def cross_series_group_by_columns(self, value: List[str]) -> None:
        self._cross_series_group_by_columns = value

    @property
    def segmentation_task_id(self) -> str:
        """
        Specifies the SegmentationTask that will be used for dividing the
        project up into multiple segmented projects.

        Notes
        -----
        segmentation_task_id : str
        """
        return self._segmentation_task_id

    @segmentation_task_id.setter
    def segmentation_task_id(self, value: str) -> None:
        self._segmentation_task_id = value

    @property
    def treat_as_exponential(self) -> str:
        """
        For time series projects only. Used to specify whether to treat data
        as exponential trend and apply transformations like log-transform. For
        classification problems `always` is not allowed.

        Notes
        -----
        treat_as_exponential : {'auto', 'never', 'always'}
        """
        return self._treat_as_exponential

    @treat_as_exponential.setter
    def treat_as_exponential(self, value: str) -> None:
        self._treat_as_exponential = value

    @property
    def use_supervised_feature_reduction(self) -> bool:
        """
        When true, during feature generation DataRobot runs a supervised
        algorithm feature lists using only qualifying features. Setting false
        can severely impact autopilot duration, especially for datasets with
        many features.

        Notes
        -----
        use_supervised_feature_reduction : bool
        """
        return self._use_supervised_feature_reduction

    @use_supervised_feature_reduction.setter
    def use_supervised_feature_reduction(self, value: bool) -> None:
        self._use_supervised_feature_reduction = value

    @property
    def use_cross_series_features(self) -> bool:
        """
        Indicating if user wants to use cross-series features.

        Notes
        -----
        use_cross_series_features : bool
        """
        return self._use_cross_series_features

    @use_cross_series_features.setter
    def use_cross_series_features(self, value: bool) -> None:
        self._use_cross_series_features = value

    @property
    def periodicities(self) -> List[FeaturesTSPeriodicityConfig]:
        """
        A list of periodicities for time series projects only. For
        classification problems periodicities are not allowed. If this is
        provided, parameter 'differencing_method' will default to 'seasonal'
        if not provided or 'auto'.

        Notes
        -----
        periodicities : list of FeaturesTSPeriodicityConfig
        """
        return self._periodicities

    @periodicities.setter
    def periodicities(self, value: List[FeaturesTSPeriodicityConfig]) -> None:
        self._periodicities = value

    @property
    def feature_settings(self) -> List[FeaturesTSFeatureSettingConfig]:
        """
        An array specifying per feature settings. Features can be left
        unspecified.

        Notes
        -----
        feature_settings : list of FeaturesTSFeatureSettingConfig
        """
        return self._feature_settings

    @feature_settings.setter
    def feature_settings(self, value: List[FeaturesTSFeatureSettingConfig]) -> None:
        self._feature_settings = value


class PartitioningDateTimeConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Date-time partitioning configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    datetime_partition_column : str
        The date column that will be used as a datetime partition column.
    validation_duration : str
        The default validation duration for all backtests. If the primary
        date/time feature in a time series project is irregular, you cannot set a
        default validation length. Instead, set each duration individually. For an
        OTV project setting the validation duration will always use regular
        partitioning. Omitting it will use irregular partitioning if the date/time
        feature is irregular.
    gap_duration : str
        The duration of the gap between holdout training and holdout scoring data.
        For time series projects, defaults to the duration of the gap between the
        end of the feature derivation window and the beginning of the forecast
        window. For OTV projects, defaults to a zero duration (P0Y0M0D).
    is_holdout_modified : bool
        A boolean value indicating whether holdout settings (start/end dates) have
        been modified by user.
    holdout_start_date : str
        The start date of holdout scoring data. When specifying
        `holdoutStartDate`, one of `holdoutEndDate` or `holdoutDuration` must also
        be specified. This attribute cannot be specified when `disableHoldout` is
        true.
    holdout_end_date : str
        The end date of holdout scoring data. When specifying `holdoutEndDate`,
        `holdoutStartDate` must also be specified. This attribute cannot be
        specified when `disableHoldout` is true.
    holdout_duration : str
        The duration of holdout scoring data. When specifying `holdoutDuration`,
        `holdoutStartDate` must also be specified. This attribute cannot be
        specified when `disableHoldout` is true.
    autopilot_data_selection_method : {'duration', 'rowCount'}
        The Data Selection method to be used by autopilot when creating models for
        datetime-partitioned datasets.
    autopilot_data_sampling_method : {'random', 'latest'}
        Defines how autopilot will select subsample from training dataset in
        OTV/TS projects. Defaults to 'latest' for 'rowCount' dataSelectionMethod
        and to 'random' for 'duration'.
    number_of_backtests : int
        The number of backtests to use. If omitted, defaults to a positive value
        selected by the server based on the validation and gap durations.
    use_project_settings : bool
        Specifies whether datetime-partitioned project should use project settings
        (i.e. backtests configuration has been modified by the user).
    backtests : list of PartitioningDTBacktestConfig
        An array specifying the format of the backtests.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "datetime_partition_column",
            "validation_duration",
            "gap_duration",
            "is_holdout_modified",
            "holdout_start_date",
            "holdout_end_date",
            "holdout_duration",
            "autopilot_data_selection_method",
            "autopilot_data_sampling_method",
            "number_of_backtests",
            "use_project_settings",
            "backtests",
        ],
        "post_multiseries_properties": ["datetime_partition_column"],
    }

    def __init__(
        self,
        datetime_partition_column: Optional[str] = None,
        validation_duration: Optional[str] = None,
        gap_duration: Optional[str] = None,
        is_holdout_modified: Optional[bool] = None,
        holdout_start_date: Optional[str] = None,
        holdout_end_date: Optional[str] = None,
        holdout_duration: Optional[str] = None,
        autopilot_data_selection_method: Optional[str] = None,
        autopilot_data_sampling_method: Optional[str] = None,
        number_of_backtests: Optional[int] = None,
        use_project_settings: Optional[bool] = None,
        backtests: Optional[List[PartitioningDTBacktestConfig]] = None,
    ) -> None:
        self.datetime_partition_column = datetime_partition_column
        self.validation_duration = validation_duration
        self.gap_duration = gap_duration
        self.is_holdout_modified = is_holdout_modified
        self.holdout_start_date = holdout_start_date
        self.holdout_end_date = holdout_end_date
        self.holdout_duration = holdout_duration
        self.autopilot_data_selection_method = autopilot_data_selection_method
        self.autopilot_data_sampling_method = autopilot_data_sampling_method
        self.number_of_backtests = number_of_backtests
        self.use_project_settings = use_project_settings
        self.backtests = backtests

    @property
    def datetime_partition_column(self) -> str:
        """
        The date column that will be used as a datetime partition column.

        Notes
        -----
        datetime_partition_column : str
        """
        return self._datetime_partition_column

    @datetime_partition_column.setter
    def datetime_partition_column(self, value: str) -> None:
        self._datetime_partition_column = value

    @property
    def validation_duration(self) -> str:
        """
        The default validation duration for all backtests. If the primary
        date/time feature in a time series project is irregular, you cannot
        set a default validation length. Instead, set each duration
        individually. For an OTV project setting the validation duration will
        always use regular partitioning. Omitting it will use irregular
        partitioning if the date/time feature is irregular.

        Notes
        -----
        validation_duration : str
        """
        return self._validation_duration

    @validation_duration.setter
    def validation_duration(self, value: str) -> None:
        self._validation_duration = value

    @property
    def gap_duration(self) -> str:
        """
        The duration of the gap between holdout training and holdout scoring
        data. For time series projects, defaults to the duration of the gap
        between the end of the feature derivation window and the beginning of
        the forecast window. For OTV projects, defaults to a zero duration
        (P0Y0M0D).

        Notes
        -----
        gap_duration : str
        """
        return self._gap_duration

    @gap_duration.setter
    def gap_duration(self, value: str) -> None:
        self._gap_duration = value

    @property
    def is_holdout_modified(self) -> bool:
        """
        A boolean value indicating whether holdout settings (start/end dates)
        have been modified by user.

        Notes
        -----
        is_holdout_modified : bool
        """
        return self._is_holdout_modified

    @is_holdout_modified.setter
    def is_holdout_modified(self, value: bool) -> None:
        self._is_holdout_modified = value

    @property
    def holdout_start_date(self) -> str:
        """
        The start date of holdout scoring data. When specifying
        `holdoutStartDate`, one of `holdoutEndDate` or `holdoutDuration` must
        also be specified. This attribute cannot be specified when
        `disableHoldout` is true.

        Notes
        -----
        holdout_start_date : str
        """
        return self._holdout_start_date

    @holdout_start_date.setter
    def holdout_start_date(self, value: str) -> None:
        self._holdout_start_date = value

    @property
    def holdout_end_date(self) -> str:
        """
        The end date of holdout scoring data. When specifying
        `holdoutEndDate`, `holdoutStartDate` must also be specified. This
        attribute cannot be specified when `disableHoldout` is true.

        Notes
        -----
        holdout_end_date : str
        """
        return self._holdout_end_date

    @holdout_end_date.setter
    def holdout_end_date(self, value: str) -> None:
        self._holdout_end_date = value

    @property
    def holdout_duration(self) -> str:
        """
        The duration of holdout scoring data. When specifying
        `holdoutDuration`, `holdoutStartDate` must also be specified. This
        attribute cannot be specified when `disableHoldout` is true.

        Notes
        -----
        holdout_duration : str
        """
        return self._holdout_duration

    @holdout_duration.setter
    def holdout_duration(self, value: str) -> None:
        self._holdout_duration = value

    @property
    def autopilot_data_selection_method(self) -> str:
        """
        The Data Selection method to be used by autopilot when creating models
        for datetime-partitioned datasets.

        Notes
        -----
        autopilot_data_selection_method : {'duration', 'rowCount'}
        """
        return self._autopilot_data_selection_method

    @autopilot_data_selection_method.setter
    def autopilot_data_selection_method(self, value: str) -> None:
        self._autopilot_data_selection_method = value

    @property
    def autopilot_data_sampling_method(self) -> str:
        """
        Defines how autopilot will select subsample from training dataset in
        OTV/TS projects. Defaults to 'latest' for 'rowCount'
        dataSelectionMethod and to 'random' for 'duration'.

        Notes
        -----
        autopilot_data_sampling_method : {'random', 'latest'}
        """
        return self._autopilot_data_sampling_method

    @autopilot_data_sampling_method.setter
    def autopilot_data_sampling_method(self, value: str) -> None:
        self._autopilot_data_sampling_method = value

    @property
    def number_of_backtests(self) -> int:
        """
        The number of backtests to use. If omitted, defaults to a positive
        value selected by the server based on the validation and gap
        durations.

        Notes
        -----
        number_of_backtests : int
        """
        return self._number_of_backtests

    @number_of_backtests.setter
    def number_of_backtests(self, value: int) -> None:
        self._number_of_backtests = value

    @property
    def use_project_settings(self) -> bool:
        """
        Specifies whether datetime-partitioned project should use project
        settings (i.e. backtests configuration has been modified by the user).

        Notes
        -----
        use_project_settings : bool
        """
        return self._use_project_settings

    @use_project_settings.setter
    def use_project_settings(self, value: bool) -> None:
        self._use_project_settings = value

    @property
    def backtests(self) -> List[PartitioningDTBacktestConfig]:
        """
        An array specifying the format of the backtests.

        Notes
        -----
        backtests : list of PartitioningDTBacktestConfig
        """
        return self._backtests

    @backtests.setter
    def backtests(self, value: List[PartitioningDTBacktestConfig]) -> None:
        self._backtests = value


class TargetConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Target configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    AutoML : dict or TargetAutoMLConfig
        AutoML additional target options
    AutoTS : dict or TargetAutoTSConfig
        Time series target(s) configuration
    target : str
        The name of the target feature.
    target_type : {'Binary', 'Regression', 'Multiclass', 'Multilabel'}
        Used to specify the targetType to use for a project when it is ambiguous,
        i.e. a numeric target with a few unique values that could be used for
        either regression or multiclass.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": ["target", "target_type"],
    }

    def __init__(
        self,
        AutoML: Optional[Union[Dict[str, Any], TargetAutoMLConfig]] = None,
        AutoTS: Optional[Union[Dict[str, Any], TargetAutoTSConfig]] = None,
        target: Optional[str] = None,
        target_type: Optional[str] = None,
    ) -> None:
        if isinstance(AutoML, TargetAutoMLConfig):
            self.AutoML = AutoML
        elif isinstance(AutoML, dict):
            self.AutoML = TargetAutoMLConfig(**AutoML)
        else:
            self.AutoML = TargetAutoMLConfig()
        self.__class__.AutoML.__doc__ = self.AutoML.__doc__

        if isinstance(AutoTS, TargetAutoTSConfig):
            self.AutoTS = AutoTS
        elif isinstance(AutoTS, dict):
            self.AutoTS = TargetAutoTSConfig(**AutoTS)
        else:
            self.AutoTS = TargetAutoTSConfig()
        self.__class__.AutoTS.__doc__ = self.AutoTS.__doc__

        self.target = target
        self.target_type = target_type

    @property
    def AutoML(self) -> TargetAutoMLConfig:
        """
        AutoML additional target options.

        Notes
        -----
        AutoML : dict or TargetAutoMLConfig
        """
        return self._AutoML

    @AutoML.setter
    def AutoML(self, value: TargetAutoMLConfig) -> None:
        self._AutoML = value

    @property
    def AutoTS(self) -> TargetAutoTSConfig:
        """
        Time series target(s) configuration.

        Notes
        -----
        AutoTS : dict or TargetAutoTSConfig
        """
        return self._AutoTS

    @AutoTS.setter
    def AutoTS(self, value: TargetAutoTSConfig) -> None:
        self._AutoTS = value

    @property
    def target(self) -> str:
        """
        The name of the target feature.

        Notes
        -----
        target : str
        """
        return self._target

    @target.setter
    def target(self, value: str) -> None:
        self._target = value

    @property
    def target_type(self) -> str:
        """
        Used to specify the targetType to use for a project when it is
        ambiguous, i.e. a numeric target with a few unique values that could
        be used for either regression or multiclass.

        Notes
        -----
        target_type : {'Binary', 'Regression', 'Multiclass', 'Multilabel'}
        """
        return self._target_type

    @target_type.setter
    def target_type(self, value: str) -> None:
        self._target_type = value


class FeaturesConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Featurization configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    AutoTS : dict or FeaturesAutoTSConfig
        Time series featurization configuration
    SAFER : dict or FeaturesSAFERConfig
        SAFER configuration
    primary_location_column : str
        Primary geospatial location column.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": ["primary_location_column"],
    }

    def __init__(
        self,
        AutoTS: Optional[Union[Dict[str, Any], FeaturesAutoTSConfig]] = None,
        SAFER: Optional[Union[Dict[str, Any], FeaturesSAFERConfig]] = None,
        primary_location_column: Optional[str] = None,
    ) -> None:
        if isinstance(AutoTS, FeaturesAutoTSConfig):
            self.AutoTS = AutoTS
        elif isinstance(AutoTS, dict):
            self.AutoTS = FeaturesAutoTSConfig(**AutoTS)
        else:
            self.AutoTS = FeaturesAutoTSConfig()
        self.__class__.AutoTS.__doc__ = self.AutoTS.__doc__

        if isinstance(SAFER, FeaturesSAFERConfig):
            self.SAFER = SAFER
        elif isinstance(SAFER, dict):
            self.SAFER = FeaturesSAFERConfig(**SAFER)
        else:
            self.SAFER = FeaturesSAFERConfig()
        self.__class__.SAFER.__doc__ = self.SAFER.__doc__

        self.primary_location_column = primary_location_column

    @property
    def AutoTS(self) -> FeaturesAutoTSConfig:
        """
        Time series featurization configuration.

        Notes
        -----
        AutoTS : dict or FeaturesAutoTSConfig
        """
        return self._AutoTS

    @AutoTS.setter
    def AutoTS(self, value: FeaturesAutoTSConfig) -> None:
        self._AutoTS = value

    @property
    def SAFER(self) -> FeaturesSAFERConfig:
        """
        SAFER configuration.

        Notes
        -----
        SAFER : dict or FeaturesSAFERConfig
        """
        return self._SAFER

    @SAFER.setter
    def SAFER(self, value: FeaturesSAFERConfig) -> None:
        self._SAFER = value

    @property
    def primary_location_column(self) -> str:
        """
        Primary geospatial location column.

        Notes
        -----
        primary_location_column : str
        """
        return self._primary_location_column

    @primary_location_column.setter
    def primary_location_column(self, value: str) -> None:
        self._primary_location_column = value


class PartitioningConfig(DrxConfig):  # type: ignore[type-arg]
    """
    Partitioning configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    User : dict or PartitioningUserConfig
        User partitioning configuration
    Group : dict or PartitioningGroupConfig
        Group partitioning configuration
    DateTime : dict or PartitioningDateTimeConfig
        Date-time partitioning configuration
    validation_type : {'CV', 'TVH'}
        The validation method to be used. CV for cross validation or TVH for
        train-validation-holdout split.
    cv_method : {'random', 'user', 'stratified', 'group', 'datetime'}
        The partitioning method to be applied to the training data.
    validation_pct : float
        The percentage of the dataset to assign to the validation set
    holdout_pct : float
        The percentage of the dataset to assign to the holdout set
    disable_holdout : bool
        Whether to suppress allocating a holdout fold. If `disableHoldout` is set
        to true, `holdoutStartDate` and `holdoutDuration` must not be set.
    reps : int
        The number of cross validation folds to use.

    See Also
    --------
    DRConfig :
        Configuration object for DataRobot project and autopilot settings,
        also includes detailed examples of usage

    """

    _rest_membership = {
        "patch_aim": [
            "validation_type",
            "cv_method",
            "validation_pct",
            "holdout_pct",
            "disable_holdout",
            "reps",
        ],
    }

    def __init__(
        self,
        User: Optional[Union[Dict[str, Any], PartitioningUserConfig]] = None,
        Group: Optional[Union[Dict[str, Any], PartitioningGroupConfig]] = None,
        DateTime: Optional[Union[Dict[str, Any], PartitioningDateTimeConfig]] = None,
        validation_type: Optional[str] = None,
        cv_method: Optional[str] = None,
        validation_pct: Optional[float] = None,
        holdout_pct: Optional[float] = None,
        disable_holdout: Optional[bool] = None,
        reps: Optional[int] = None,
    ) -> None:
        if isinstance(User, PartitioningUserConfig):
            self.User = User
        elif isinstance(User, dict):
            self.User = PartitioningUserConfig(**User)
        else:
            self.User = PartitioningUserConfig()
        self.__class__.User.__doc__ = self.User.__doc__

        if isinstance(Group, PartitioningGroupConfig):
            self.Group = Group
        elif isinstance(Group, dict):
            self.Group = PartitioningGroupConfig(**Group)
        else:
            self.Group = PartitioningGroupConfig()
        self.__class__.Group.__doc__ = self.Group.__doc__

        if isinstance(DateTime, PartitioningDateTimeConfig):
            self.DateTime = DateTime
        elif isinstance(DateTime, dict):
            self.DateTime = PartitioningDateTimeConfig(**DateTime)
        else:
            self.DateTime = PartitioningDateTimeConfig()
        self.__class__.DateTime.__doc__ = self.DateTime.__doc__

        self.validation_type = validation_type
        self.cv_method = cv_method
        self.validation_pct = validation_pct
        self.holdout_pct = holdout_pct
        self.disable_holdout = disable_holdout
        self.reps = reps

    @property
    def User(self) -> PartitioningUserConfig:
        """
        User partitioning configuration.

        Notes
        -----
        User : dict or PartitioningUserConfig
        """
        return self._User

    @User.setter
    def User(self, value: PartitioningUserConfig) -> None:
        self._User = value

    @property
    def Group(self) -> PartitioningGroupConfig:
        """
        Group partitioning configuration.

        Notes
        -----
        Group : dict or PartitioningGroupConfig
        """
        return self._Group

    @Group.setter
    def Group(self, value: PartitioningGroupConfig) -> None:
        self._Group = value

    @property
    def DateTime(self) -> PartitioningDateTimeConfig:
        """
        Date-time partitioning configuration.

        Notes
        -----
        DateTime : dict or PartitioningDateTimeConfig
        """
        return self._DateTime

    @DateTime.setter
    def DateTime(self, value: PartitioningDateTimeConfig) -> None:
        self._DateTime = value

    @property
    def validation_type(self) -> str:
        """
        The validation method to be used. CV for cross validation or TVH for
        train-validation-holdout split.

        Notes
        -----
        validation_type : {'CV', 'TVH'}
        """
        return self._validation_type

    @validation_type.setter
    def validation_type(self, value: str) -> None:
        self._validation_type = value

    @property
    def cv_method(self) -> str:
        """
        The partitioning method to be applied to the training data.

        Notes
        -----
        cv_method : {'random', 'user', 'stratified', 'group', 'datetime'}
        """
        return self._cv_method

    @cv_method.setter
    def cv_method(self, value: str) -> None:
        self._cv_method = value

    @property
    def validation_pct(self) -> float:
        """
        The percentage of the dataset to assign to the validation set.

        Notes
        -----
        validation_pct : float
        """
        return self._validation_pct

    @validation_pct.setter
    def validation_pct(self, value: float) -> None:
        self._validation_pct = value

    @property
    def holdout_pct(self) -> float:
        """
        The percentage of the dataset to assign to the holdout set.

        Notes
        -----
        holdout_pct : float
        """
        return self._holdout_pct

    @holdout_pct.setter
    def holdout_pct(self, value: float) -> None:
        self._holdout_pct = value

    @property
    def disable_holdout(self) -> bool:
        """
        Whether to suppress allocating a holdout fold. If `disableHoldout` is
        set to true, `holdoutStartDate` and `holdoutDuration` must not be set.

        Notes
        -----
        disable_holdout : bool
        """
        return self._disable_holdout

    @disable_holdout.setter
    def disable_holdout(self, value: bool) -> None:
        self._disable_holdout = value

    @property
    def reps(self) -> int:
        """
        The number of cross validation folds to use.

        Notes
        -----
        reps : int
        """
        return self._reps

    @reps.setter
    def reps(self, value: int) -> None:
        self._reps = value


class DRConfig(DrxConfig):  # type: ignore[type-arg]
    """
    DataRobot configuration.

    Parameters that default to 'None' (or are omitted by the user) are overridden
    to server-side defaults at runtime. Consult the DataRobot REST API and GUI
    documentation for additional information on each parameter.

    Parameters
    ----------
    Data : dict or DataConfig
        Row and column selection configuration
    Target : dict or TargetConfig
        Target configuration
    Featurization : dict or FeaturesConfig
        Featurization configuration
    Partitioning : dict or PartitioningConfig
        Partitioning configuration
    Modeling : dict or ModelingConfig
        Modeling configuration
    Metadata : dict or MetadataConfig
        DataRobot metadata and worker configuration

    Examples
    --------
    Direct configuration object construction

    >>> from datarobotx import DRConfig
    >>> config_1 = DRConfig()

    Configuration object construction via abstraction get_params()

    >>> from datarobotx.models.automl import AutoMLModel
    >>> model_1 = AutoMLModel(name='elated-varahamihira')
    >>> config_2 = model.get_params()

    Dictionary representation of configuration

    >>> config_2
    {'project_description': 'my_description', 'project_name': 'elated-varahamihira'}

    Object specification through nested attributes (useful with autocomplete discovery)

    >>> config_2.Metadata.project_description = 'my_description'

    Configuration of abstractions using a config object, dictionary, or keyword arguments

    >>> model_1.set_params(**config_2).get_params()
    {'project_description': 'my_description', 'project_name': 'elated-varahamihira'}
    >>> my_dict = {'project_description': 'my_new_description'}
    >>> model_1.set_params(**my_dict).get_params()
    {'project_description': 'my_new_description', 'project_name': 'elated-varahamihira'}
    >>> model_1.set_params(project_name='my_new_name').get_params()
    {'project_description': 'my_new_description', 'project_name': 'my_new_name'}


    Construction of abstractions using a configuration object

    >>> from datarobotx.models.automl import AutoMLModel
    >>> model_2 = AutoMLModel(**config_2)
    >>> model_2.get_params()
    {'project_description': 'my_description', 'project_name': 'elated-varahamihira'}


    """

    _rest_membership = {}

    def __init__(
        self,
        Data: Optional[Union[Dict[str, Any], DataConfig]] = None,
        Target: Optional[Union[Dict[str, Any], TargetConfig]] = None,
        Featurization: Optional[Union[Dict[str, Any], FeaturesConfig]] = None,
        Partitioning: Optional[Union[Dict[str, Any], PartitioningConfig]] = None,
        Modeling: Optional[Union[Dict[str, Any], ModelingConfig]] = None,
        Metadata: Optional[Union[Dict[str, Any], MetadataConfig]] = None,
    ) -> None:
        if isinstance(Data, DataConfig):
            self.Data = Data
        elif isinstance(Data, dict):
            self.Data = DataConfig(**Data)
        else:
            self.Data = DataConfig()
        self.__class__.Data.__doc__ = self.Data.__doc__

        if isinstance(Target, TargetConfig):
            self.Target = Target
        elif isinstance(Target, dict):
            self.Target = TargetConfig(**Target)
        else:
            self.Target = TargetConfig()
        self.__class__.Target.__doc__ = self.Target.__doc__

        if isinstance(Featurization, FeaturesConfig):
            self.Featurization = Featurization
        elif isinstance(Featurization, dict):
            self.Featurization = FeaturesConfig(**Featurization)
        else:
            self.Featurization = FeaturesConfig()
        self.__class__.Featurization.__doc__ = self.Featurization.__doc__

        if isinstance(Partitioning, PartitioningConfig):
            self.Partitioning = Partitioning
        elif isinstance(Partitioning, dict):
            self.Partitioning = PartitioningConfig(**Partitioning)
        else:
            self.Partitioning = PartitioningConfig()
        self.__class__.Partitioning.__doc__ = self.Partitioning.__doc__

        if isinstance(Modeling, ModelingConfig):
            self.Modeling = Modeling
        elif isinstance(Modeling, dict):
            self.Modeling = ModelingConfig(**Modeling)
        else:
            self.Modeling = ModelingConfig()
        self.__class__.Modeling.__doc__ = self.Modeling.__doc__

        if isinstance(Metadata, MetadataConfig):
            self.Metadata = Metadata
        elif isinstance(Metadata, dict):
            self.Metadata = MetadataConfig(**Metadata)
        else:
            self.Metadata = MetadataConfig()
        self.__class__.Metadata.__doc__ = self.Metadata.__doc__

    @property
    def Data(self) -> DataConfig:
        """
        Row and column selection configuration.

        Notes
        -----
        Data : dict or DataConfig
        """
        return self._Data

    @Data.setter
    def Data(self, value: DataConfig) -> None:
        self._Data = value

    @property
    def Target(self) -> TargetConfig:
        """
        Target configuration.

        Notes
        -----
        Target : dict or TargetConfig
        """
        return self._Target

    @Target.setter
    def Target(self, value: TargetConfig) -> None:
        self._Target = value

    @property
    def Featurization(self) -> FeaturesConfig:
        """
        Featurization configuration.

        Notes
        -----
        Featurization : dict or FeaturesConfig
        """
        return self._Featurization

    @Featurization.setter
    def Featurization(self, value: FeaturesConfig) -> None:
        self._Featurization = value

    @property
    def Partitioning(self) -> PartitioningConfig:
        """
        Partitioning configuration.

        Notes
        -----
        Partitioning : dict or PartitioningConfig
        """
        return self._Partitioning

    @Partitioning.setter
    def Partitioning(self, value: PartitioningConfig) -> None:
        self._Partitioning = value

    @property
    def Modeling(self) -> ModelingConfig:
        """
        Modeling configuration.

        Notes
        -----
        Modeling : dict or ModelingConfig
        """
        return self._Modeling

    @Modeling.setter
    def Modeling(self, value: ModelingConfig) -> None:
        self._Modeling = value

    @property
    def Metadata(self) -> MetadataConfig:
        """
        DataRobot metadata and worker configuration.

        Notes
        -----
        Metadata : dict or MetadataConfig
        """
        return self._Metadata

    @Metadata.setter
    def Metadata(self, value: MetadataConfig) -> None:
        self._Metadata = value
