_target_: lightning.pytorch.trainer.Trainer

default_root_dir: ${paths.output_dir}

min_epochs: 1 # prevents early stopping
max_epochs: 20

accelerator: gpu
devices: 1
num_nodes: 1

# mixed precision for extra speed-up
# precision: bf16-mixed

# perform a validation loop every N training epochs
check_val_every_n_epoch: 1

# set True to to ensure deterministic results
# makes training slower but gives more reproducibility than just setting seeds
deterministic: False

# NOTE: we clip by value instead of norm since Lightning does not support norm clipping for FSDP yet, per https://github.com/Lightning-AI/pytorch-lightning/issues/19235
gradient_clip_algorithm: value
# if `gradient_clip_val` is not `0.0` or `null`, gradients will be value-clipped by an upper bound of `gradient_clip_val` during training
gradient_clip_val: null

# accumulate gradients of `accumulate_grad_batches` batches (e.g., by this amount for each rank) before doing a backward pass
# NOTE: here, we would have to accumulate gradients independently for each rank, per https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#accumulate-gradients
accumulate_grad_batches: 1

# if `num_sanity_val_steps` is > 0, then specifically that many validation steps will be run during the first call to `trainer.fit`
num_sanity_val_steps: 0
