
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from typing import List
from MPSPlots.styles import mps
from FlowCyPy.units import second
from FlowCyPy.joint_plot import JointPlotWithMarginals
import warnings
import logging
from tabulate import tabulate
from FlowCyPy.report import Report


class Analyzer:
    """
    A class to analyze pulse signals generated by a flow cytometer, extracting features
    such as pulse height, width, and area from the signal.

    Attributes
    ----------
    detectors : List[object]
        List of detector objects for analysis.

    Methods
    -------
    run_analysis(compute_peak_area=True, height_threshold=None):
        Detects and extracts features from signals.
    get_coincidence(coincidence_margin=0.1):
        Returns dataframe where peak times from different detectors match within a given margin.
    find_peaks(detector, peak_area=True, height_threshold=None):
        Detects significant peaks and calculates their features.
    display_features():
        Displays detected features in tabular format.
    plot():
        Plots signals along with detected peaks.
    """

    def __init__(self, cytometer, algorithm: object) -> None:
        """
        Initializes the PeakAnalyzer with a list of detectors.

        Parameters
        ----------
        detectors : List[object]
            A list of detector objects that contain time and signal data.
        """
        self.algorithm = algorithm
        self.cytometer = cytometer
        detectors = cytometer.detectors

        if len(detectors) != 2:
            raise ValueError("PeakAnalyzer currently supports exactly two detectors.")

        self.detectors = detectors
        self.datasets = []


    def run_analysis(self, compute_peak_area: bool = False) -> pd.DataFrame:
        """
        Main method to run the peak analysis on all detectors and extract features like heights, widths,
        areas, and other key statistics.

        Parameters
        ----------
        compute_peak_area : bool, optional
            Whether to compute the area under the peaks, by default False.

        Returns
        -------
        pd.DataFrame
            MultiIndex DataFrame with peak properties for each detector.
        """
        logging.info("Starting peak analysis for all detectors.")

        # Run peak detection on each detector
        combined_results = self._analyze_all_detectors(compute_peak_area)

        # Calculate and log additional statistics
        self._log_statistics(combined_results)

        return combined_results

    def _analyze_detector(self, detector, detector_index: int, compute_peak_area: bool) -> pd.DataFrame:
        """
        Analyzes a single detector by running the peak detection algorithm and extracting peak properties.

        Parameters
        ----------
        detector : object
            The detector object containing signal and time data.
        detector_index : int
            The index of the detector for logging purposes.
        compute_peak_area : bool
            Whether to compute the area under the peaks.

        Returns
        -------
        pd.DataFrame
            DataFrame containing the peak properties for the analyzed detector.
        """
        logging.info(f"Analyzing Detector {detector_index + 1}.")

        # Run the peak detection algorithm
        self.algorithm.detect_peaks(detector=detector, compute_area=compute_peak_area)

        logging.info(f"Detector {detector_index + 1}: Detected {len(detector.peak_properties)} peaks.")

    def _analyze_all_detectors(self, compute_peak_area: bool) -> pd.DataFrame:
        """
        Runs the peak detection on all detectors and returns the combined results.

        Parameters
        ----------
        compute_peak_area : bool
            Whether to compute the area under the peaks.

        Returns
        -------
        pd.DataFrame
            MultiIndex DataFrame with peak properties for all detectors.
        """
        all_results = []
        for i, detector in enumerate(self.detectors):
            # Analyze each detector and append the results
            self._analyze_detector(detector, i, compute_peak_area)

        n_col = range(len(self.detectors))

        combined_results = pd.concat(
            [d.peak_properties for d in self.detectors],
            keys=[f'Detector_{i + 1}' for i in n_col]
        )

        combined_results.index.names = ['Detector', 'Event']
        self.dataframe = combined_results

        return combined_results

    def _log_statistics(self, combined_results: pd.DataFrame):
        """
        Logs key statistics about the detected peaks for each detector using tabulate for better formatting.
        Includes total events, average time between peaks, first and last peak times, and other relevant info.

        Parameters
        ----------
        combined_results : pd.DataFrame
            The combined DataFrame containing peak properties for all detectors.
        """
        total_peaks = 0
        table_data = []  # List to store table data for each detector

        logging.info("\n=== Analysis Summary ===")

        # Group by the 'Detector' column to calculate stats for each detector
        for detector, (_, group) in zip(self.detectors, combined_results.groupby('Detector')):
            num_events = len(group)
            total_peaks += num_events

            # Calculate average time between peaks if more than one event is detected
            if num_events > 1:
                times = group['PeakTimes'].sort_values()
                avg_time_between_peaks = times.diff().mean()
            else:
                avg_time_between_peaks = "N/A"

            # Get first and last peak times
            first_peak_time = group['PeakTimes'].min() if num_events > 0 else "N/A"
            last_peak_time = group['PeakTimes'].max() if num_events > 0 else "N/A"

            # Append detector statistics to table data
            table_data.append([
                detector.name,                     # Detector name
                num_events,                        # Number of events
                f"{first_peak_time:.3~P}",         # First peak time
                f"{last_peak_time:.3~P}",          # Last peak time
                f"{avg_time_between_peaks:.3~P}"   # Average time between peaks
            ])

        # Format the table using tabulate
        headers = ["Detector", "Number of Events", "First Peak Time", "Last Peak Time", "Avg Time Between Peaks"]
        formatted_table = tabulate(table_data, headers=headers, tablefmt="grid", floatfmt=".3f")

        # Log the formatted table
        logging.info("\n" + formatted_table)

        # Log total peaks across all detectors
        logging.info(f"\nTotal number of peaks detected across all detectors: {total_peaks}")

    def get_coincidence(self, margin: second.dimensionality) -> pd.DataFrame:
        """
        Identifies coincident events between two detectors within a specified time margin.

        Parameters
        ----------
        dataframe : pd.DataFrame
            DataFrame containing peak times for detectors.
        margin : pint.Quantity
            Time margin within which peaks are considered coincident, in compatible time units.

        Returns
        -------
        pd.DataFrame
            DataFrame with coincident events from both detectors.
        """

        # Ensure margin has correct dimensionality (time)
        assert margin.dimensionality == second.dimensionality, "Margin must have time dimensionality."

        self.dataframe['PeakTimes'] = self.dataframe['PeakTimes'].pint.to(margin.units)

        # Split the data for Detector_1 and Detector_2
        d0 = self.dataframe.xs('Detector_1', level='Detector')
        d1 = self.dataframe.xs('Detector_2', level='Detector')

        # Repeat and tile PeakTimes for comparison (keeping your protocol)
        d0_repeated = np.repeat(d0.index.values.numpy_data, len(d1)) * margin.units
        d1_tiled = np.tile(d1.index.values.numpy_data, len(d0)) * margin.units

        # Compute time differences and reshape the mask
        time_diffs = np.abs(d0_repeated - d1_tiled)
        mask = time_diffs <= margin
        mask = mask.reshape(len(d0), len(d1))

        # Find indices where coincidences occur
        indices = np.where(mask)

        # Count coincidences per column (for each event in Detector_1)
        true_count_per_column = np.sum(mask.astype(int), axis=0)

        # Warnings and assertions
        if np.all(true_count_per_column == 0):
            warnings.warn("No coincidence events found, the margin might be too low.")

        assert np.all(true_count_per_column <= 1), \
            "Coincidence events are ambiguously defined, the margin might be too high."

        # Extract coincident events from both detectors
        coincident_detector_0 = d0.iloc[indices[0]].reset_index(drop=True)
        coincident_detector_1 = d1.iloc[indices[1]].reset_index(drop=True)

        # Combine the coincident events into a single DataFrame
        combined_coincidences = pd.concat([coincident_detector_0, coincident_detector_1], axis=1)

        # Assign proper MultiIndex column names
        combined_coincidences.columns = pd.MultiIndex.from_product([['Detector_1', 'Detector_2'], d0.columns])

        self.coincidence = combined_coincidences

        return self.coincidence

    def display_features(self) -> None:
        """
        Displays extracted peak features for all datasets in a tabular format.
        """
        for i, dataset in enumerate(self.datasets):
            print(f"\nFeatures for Dataset {i + 1}:")
            dataset.print_properties()  # Reuse the print_properties method from DataSet


    def plot_peak(self, show: bool = True, figure_size: tuple = (7, 6)) -> None:
        """
        Plots the signal with detected peaks and widths at half-maximum, if available.
        """
        with plt.style.context(mps):

            fig, axes = plt.subplots(
                ncols=1,
                nrows=len(self.detectors),
                figsize=(figure_size[0], figure_size[1]),
                sharex=True
            )

            for ax, detector in zip(axes, self.detectors):
                self.algorithm.plot(detector, ax=ax, show=False)

            plt.tight_layout()

            if show:
                plt.show()

    def plot(self, show: bool = True) -> None:
        """
        Plots the 2D density plot of the scattering intensities from the two detectors.

        The plot includes:
            - A 2D hexbin density plot.
            - X-axis label positioned on top and y-axis label positioned on the right.
            - A horizontal colorbar at the bottom indicating the density.
        """
        with plt.style.context(mps):
            d0 = self.coincidence['Detector_1']
            d1 = self.coincidence['Detector_2']


            joint_plot = JointPlotWithMarginals(
                xlabel=f'Detector {self.detectors[0].name} Scattering Intensity [{d0.Heights.values.units}]',
                ylabel=f'Detector {self.detectors[1].name} Scattering Intensity [{d1.Heights.values.units}]'
            )

            joint_plot.add_dataset(x=d0.Heights.values, y=d1.Heights.values, alpha=0.7)

            joint_plot.generate()

            if show:
                joint_plot.show()

    def generate_report(self, filename: str) -> None:
        report = Report(
            flow=self.cytometer.scatterer_distribution.flow,
            scatterer_distribution=self.cytometer.scatterer_distribution,
            analyzer=self
        )

        report.generate_report()