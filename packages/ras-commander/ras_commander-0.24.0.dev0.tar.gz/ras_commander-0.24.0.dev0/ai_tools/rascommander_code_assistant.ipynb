{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAS-Commander Code Assistant\n",
    "\n",
    "Alpha, this only works with Claude 3.5 Sonnet for now\n",
    "\n",
    "Future devlopement will include multi-turn support and ability to select between different models\n",
    "\n",
    "Provide your own API key to make this work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User query\n",
    "user_query = \"\"\"   Make a table for each class file in the library with all functions, their arguments (with typing/expected input), and a short summary of the function's purpose.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define files, folders and extensions to omit\n",
    "omit_folders = [\n",
    "    \"Bald Eagle Creek\", \n",
    "    \"__pycache__\", \n",
    "    \".git\", \n",
    "    \".github\", \n",
    "    \"tests\", \n",
    "    \"build\", \n",
    "    \"dist\", \n",
    "    \"ras_commander.egg-info\", \n",
    "    \"venv\", \n",
    "    \"example_projects\", \n",
    "    \"llm_summary\", \"misc\", \"future\", \"ai_tools\"\n",
    "]\n",
    "\n",
    "# Define file extensions to omit\n",
    "omit_extensions = [\n",
    "    # Common image file extensions\n",
    "    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp', '.svg', '.ico',\n",
    "    # Other binary file extensions\n",
    "    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',\n",
    "    '.zip', '.rar', '.7z', '.tar', '.gz',\n",
    "    '.exe', '.dll', '.so', '.dylib',\n",
    "    '.pyc', '.pyo', '.pyd',  # Python bytecode and compiled files\n",
    "    '.class',  # Java bytecode\n",
    "    '.log', '.tmp', '.bak', '.swp',  # Temporary and backup files\n",
    "    '.bat', '.sh',  # Script files\n",
    "]\n",
    "\n",
    "# Define files to omit based on keywords\n",
    "omit_files = [\n",
    "    'FunctionList.md',\n",
    "    'DS_Store',\n",
    "    'Thumbs.db',\n",
    "    'llmsummarize'\n",
    "    'example_projects.zip',\n",
    "    '11_accessing_example_projects.ipynb',\n",
    "    'Example_Projects_6_5.zip'\n",
    "    'github_code_assistant.ipynb',\n",
    "    'example_projects.ipynb',\n",
    "    '11_Using_RasExamples.ipynb',\n",
    "    'example_projects.csv',\n",
    "    'rascommander_code_assistant.ipynb',\n",
    "    'RasExamples.py'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported pandas\n",
      "Successfully imported anthropic\n",
      "Successfully imported tiktoken\n",
      "Successfully imported IPython.display\n",
      "Successfully imported astor\n",
      "All required packages have been installed and imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "def install_and_import(package_name, import_name=None):\n",
    "    import subprocess\n",
    "    import sys\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        __import__(import_name)\n",
    "    print(f\"Successfully imported {import_name}\")\n",
    "\n",
    "install_and_import(\"pandas\")\n",
    "install_and_import(\"anthropic\")\n",
    "install_and_import(\"tiktoken\")\n",
    "install_and_import(\"IPython\", \"IPython.display\")\n",
    "install_and_import(\"astor\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import anthropic\n",
    "import tiktoken\n",
    "import astor\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(\"All required packages have been installed and imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Key\n",
    "#os.environ[\"ANTHROPIC_API_KEY\"] = 'YOUR KEY HERE'\n",
    "\n",
    "\n",
    "# Alternately, you can set the key from the file Anthropic_API_Key.txt\n",
    "with open('C:\\SCRATCH\\Anthropic_API_Key.txt', 'r') as file:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System message loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define system message from the ras_commander .cursorrules file\n",
    "from pathlib import Path\n",
    "\n",
    "def read_system_message():\n",
    "    # Get the current notebook's directory\n",
    "    current_dir = Path.cwd()\n",
    "    \n",
    "    # Path to the .cursorrules file (assuming it's in the parent directory)\n",
    "    cursor_rules_path = current_dir.parent / '.cursorrules'\n",
    "\n",
    "    # Check if .cursorrules exists\n",
    "    if not cursor_rules_path.exists():\n",
    "        raise FileNotFoundError(\"This notebook expects to be in a directory within the ras_commander repo which has a .cursorrules file in its parent directory.\")\n",
    "\n",
    "    # Read the .cursorrules file as plain text\n",
    "    with open(cursor_rules_path, 'r') as f:\n",
    "        system_message = f.read().strip()\n",
    "\n",
    "    if not system_message:\n",
    "        raise ValueError(\"No system message found in .cursorrules file.\")\n",
    "\n",
    "    return system_message\n",
    "\n",
    "# Read the system message from .cursorrules\n",
    "system_message = read_system_message()\n",
    "\n",
    "print(\"System message loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context folder set to: c:\\GH\\ras_commander\n"
     ]
    }
   ],
   "source": [
    "# Define folder as the parent folder (since this notebook lives in the ai_tools folder)\n",
    "# Get the current notebook's directory\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Set the context folder to the parent of the current directory\n",
    "context_folder = current_dir.parent\n",
    "\n",
    "print(f\"Context folder set to: {context_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook name: ai_tools\n",
      "Subfolder to summarize: c:\\GH\\ras_commander\n",
      "Output file path: c:\\GH\\ras_commander\\llm_summary\\ras_commander_code_only.txt\n",
      "Output directory ensured to exist: c:\\GH\\ras_commander\\llm_summary\n",
      "Opened output file: c:\\GH\\ras_commander\\llm_summary\\ras_commander_code_only.txt\n",
      "All files and folders have been combined into 'c:\\GH\\ras_commander\\llm_summary\\ras_commander_code_only.txt'\n",
      "Notebook name: ai_tools\n",
      "Subfolder to summarize: c:\\GH\\ras_commander\n",
      "Output file path: c:\\GH\\ras_commander\\llm_summary\\ras_commander_code_only_stripped.txt\n",
      "Output directory ensured to exist: c:\\GH\\ras_commander\\llm_summary\n",
      "Opened output file: c:\\GH\\ras_commander\\llm_summary\\ras_commander_code_only_stripped.txt\n",
      "All files and folders have been combined into 'c:\\GH\\ras_commander\\llm_summary\\ras_commander_code_only_stripped.txt'\n"
     ]
    }
   ],
   "source": [
    "# Function to compile codebase, omitting specified folders, extensions, and files\n",
    "\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "\n",
    "def strip_code_from_functions(content):\n",
    "    \"\"\"\n",
    "    Strip the code from functions, leaving only function signatures and docstrings.\n",
    "    \n",
    "    Args:\n",
    "    content (str): The content of a Python file.\n",
    "    \n",
    "    Returns:\n",
    "    str: The content with function bodies removed.\n",
    "    \"\"\"\n",
    "    import ast\n",
    "    import astor\n",
    "\n",
    "    class FunctionStripper(ast.NodeTransformer):\n",
    "        def visit_FunctionDef(self, node):\n",
    "            # Keep the function signature and docstring (if present)\n",
    "            new_node = ast.FunctionDef(\n",
    "                name=node.name,\n",
    "                args=node.args,\n",
    "                body=[ast.Pass()],  # Replace the body with a pass statement\n",
    "                decorator_list=node.decorator_list,\n",
    "                returns=node.returns\n",
    "            )\n",
    "            # If there's a docstring, keep it\n",
    "            if (len(node.body) > 0 and isinstance(node.body[0], ast.Expr) and\n",
    "                isinstance(node.body[0].value, ast.Str)):\n",
    "                new_node.body = [node.body[0], ast.Pass()]\n",
    "            return new_node\n",
    "\n",
    "    try:\n",
    "        tree = ast.parse(content)\n",
    "        stripped_tree = FunctionStripper().visit(tree)\n",
    "        return astor.to_source(stripped_tree)\n",
    "    except SyntaxError:\n",
    "        # If parsing fails, return the original content\n",
    "        return content\n",
    "\n",
    "def combine_files(summarize_subfolder, omit_folders, omit_extensions, omit_files, strip_code=False):\n",
    "    combined_text = \"\"\n",
    "    file_token_counts = {}\n",
    "    \n",
    "    # Get the name of this notebook\n",
    "    this_notebook = Path.cwd().name\n",
    "    print(f\"Notebook name: {this_notebook}\")\n",
    "\n",
    "    # Ensure summarize_subfolder is a Path object\n",
    "    summarize_subfolder = Path(summarize_subfolder)\n",
    "    print(f\"Subfolder to summarize: {summarize_subfolder}\")\n",
    "\n",
    "    # Define the output file name based on the folder name\n",
    "    output_file_name = f\"{summarize_subfolder.name}_code_only{'_stripped' if strip_code else ''}.txt\"\n",
    "    output_file_path = Path.cwd().parent / \"llm_summary\" / output_file_name\n",
    "    print(f\"Output file path: {output_file_path}\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Output directory ensured to exist: {output_file_path.parent}\")\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Open the output file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        print(f\"Opened output file: {output_file_path}\")\n",
    "        # Iterate over all files and subfolders in the summarize_subfolder directory\n",
    "        for filepath in summarize_subfolder.rglob('*'):\n",
    "            # Check if the file is not this notebook, not in the omit_folders, not in omit_extensions, and not in omit_files\n",
    "            if (filepath.name != this_notebook and \n",
    "                not any(omit_folder in filepath.parts for omit_folder in omit_folders) and\n",
    "                filepath.suffix.lower() not in omit_extensions and\n",
    "                not any(omit_file in filepath.name for omit_file in omit_files)):\n",
    "                # Write the filename or folder name\n",
    "                if filepath.is_file():\n",
    "                    outfile.write(f\"File: {filepath}\\n\")\n",
    "                else:\n",
    "                    outfile.write(f\"Folder: {filepath}\\n\")\n",
    "                outfile.write(\"=\"*50 + \"\\n\")  # Separator\n",
    "                \n",
    "                # If it's a file, open and read the contents of the file\n",
    "                if filepath.is_file():\n",
    "                    try:\n",
    "                        with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "                            content = infile.read()\n",
    "                    except UnicodeDecodeError:\n",
    "                        with open(filepath, 'rb') as infile:\n",
    "                            content = infile.read()\n",
    "                            content = content.decode('utf-8', errors='ignore')\n",
    "                    \n",
    "                    # Strip code if the option is enabled and it's a Python file\n",
    "                    if strip_code and filepath.suffix.lower() == '.py':\n",
    "                        content = strip_code_from_functions(content)\n",
    "                    \n",
    "                    # Write the contents to the output file\n",
    "                    outfile.write(content)\n",
    "                    \n",
    "                    # Count tokens for this file\n",
    "                    file_tokens = len(enc.encode(content))\n",
    "                    file_token_counts[str(filepath)] = file_tokens\n",
    "                \n",
    "                # Write a separator after the file contents or folder name\n",
    "                outfile.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "            else:\n",
    "                dummy = 0\n",
    "\n",
    "    print(f\"All files and folders have been combined into '{output_file_path}'\")\n",
    "\n",
    "    # Count total tokens\n",
    "    with open(output_file_path, 'r', encoding='utf-8') as f:\n",
    "        combined_text = f.read()\n",
    "    token_count = len(enc.encode(combined_text))\n",
    "    \n",
    "    return combined_text, token_count, file_token_counts\n",
    "\n",
    "\n",
    "# Combine files while keeping code\n",
    "combined_text, token_count, file_token_counts = combine_files(context_folder, omit_folders, omit_extensions, omit_files)\n",
    "\n",
    "# Combine files while stripping code\n",
    "combined_text, token_count, file_token_counts = combine_files(context_folder, omit_folders, omit_extensions, omit_files, strip_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 files by token count:\n",
      "c:\\GH\\ras_commander\\future_dev_roadmap.ipynb: 7807 tokens\n",
      "c:\\GH\\ras_commander\\ras_commander\\RasPlan.py: 3796 tokens\n",
      "c:\\GH\\ras_commander\\Comprehensive_Library_Guide.md: 2391 tokens\n",
      "c:\\GH\\ras_commander\\README.md: 2071 tokens\n",
      "c:\\GH\\ras_commander\\ras_commander\\RasPrj.py: 1711 tokens\n",
      "c:\\GH\\ras_commander\\ras_commander\\RasUtils.py: 1507 tokens\n",
      "c:\\GH\\ras_commander\\STYLE_GUIDE.md: 1461 tokens\n",
      "c:\\GH\\ras_commander\\ras_commander\\RasCommander.py: 1392 tokens\n",
      "c:\\GH\\ras_commander\\ras_commander\\README.md: 1250 tokens\n",
      "c:\\GH\\ras_commander\\examples\\14_Core_Sensitivity.ipynb: 1174 tokens\n",
      "c:\\GH\\ras_commander\\.cursorrules: 1107 tokens\n",
      "c:\\GH\\ras_commander\\examples\\02_plan_operations.py: 484 tokens\n",
      "c:\\GH\\ras_commander\\ras_commander\\RasGeo.py: 387 tokens\n",
      "c:\\GH\\ras_commander\\ras_commander\\__init__.py: 241 tokens\n",
      "c:\\GH\\ras_commander\\pyproject.toml: 239 tokens\n",
      "c:\\GH\\ras_commander\\examples\\07_sequential_plan_execution.py: 239 tokens\n",
      "c:\\GH\\ras_commander\\examples\\09_specifying_plans.py: 239 tokens\n",
      "c:\\GH\\ras_commander\\updated_pyproject.toml: 198 tokens\n",
      "c:\\GH\\ras_commander\\ras_commander\\RasUnsteady.py: 196 tokens\n",
      "c:\\GH\\ras_commander\\examples\\13_multiple_project_operations.py: 194 tokens\n"
     ]
    }
   ],
   "source": [
    "# Sort files by token count and get top 20\n",
    "top_20_files = sorted(file_token_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "print(\"\\nTop 20 files by token count:\")\n",
    "for file, count in top_20_files:\n",
    "    print(f\"{file}: {count} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text token count: 30526\n"
     ]
    }
   ],
   "source": [
    "# Check the total token count\n",
    "print(f\"Combined text token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pricing DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Input ($/MTok)</th>\n",
       "      <th>Output ($/MTok)</th>\n",
       "      <th>Prompt Caching Write ($/MTok)</th>\n",
       "      <th>Prompt Caching Read ($/MTok)</th>\n",
       "      <th>Context Window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Claude 3.5 Sonnet</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.3</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Input ($/MTok)  Output ($/MTok)  \\\n",
       "0  Claude 3.5 Sonnet               3               15   \n",
       "\n",
       "   Prompt Caching Write ($/MTok)  Prompt Caching Read ($/MTok)  Context Window  \n",
       "0                           3.75                           0.3          200000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated cost: $0.2146\n"
     ]
    }
   ],
   "source": [
    "# Set up Anthropic client\n",
    "def stream_response(client, full_prompt, max_tokens=4096):\n",
    "    response_text = \"\"\n",
    "    with client.messages.stream(\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ],\n",
    "        model=\"claude-3-sonnet-20240229\"\n",
    "    ) as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response_text += text\n",
    "            clear_output(wait=True)\n",
    "            print(\"Claude's response:\")\n",
    "            print(response_text)\n",
    "            \n",
    "    return response_text\n",
    "\n",
    "def estimate_cost(input_tokens, output_tokens, pricing_df):\n",
    "    model = \"Claude 3.5 Sonnet\"\n",
    "    input_cost = (input_tokens / 1e6) * pricing_df.loc[pricing_df['Model'] == model, 'Input ($/MTok)'].values[0]\n",
    "    output_cost = (output_tokens / 1e6) * pricing_df.loc[pricing_df['Model'] == model, 'Output ($/MTok)'].values[0]\n",
    "    return input_cost + output_cost\n",
    "\n",
    "\n",
    "# Set up Anthropic client\n",
    "client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Create pricing dataframe\n",
    "pricing_data = {\n",
    "    \"Model\": [\"Claude 3.5 Sonnet\"],\n",
    "    \"Input ($/MTok)\": [3],\n",
    "    \"Output ($/MTok)\": [15],\n",
    "    \"Prompt Caching Write ($/MTok)\": [3.75],\n",
    "    \"Prompt Caching Read ($/MTok)\": [0.30],\n",
    "    \"Context Window\": [200000]\n",
    "}\n",
    "\n",
    "pricing_df = pd.DataFrame(pricing_data)\n",
    "print(\"Pricing DataFrame:\")\n",
    "display(pricing_df)\n",
    "\n",
    "# Combine system message, context, and user query\n",
    "full_prompt = f\"{system_message}\\n\\nContext:\\n{combined_text}\\n\\n\\n\\n\\nUser Query: {user_query}\"\n",
    "\n",
    "# Estimate cost\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "input_tokens = token_count + len(enc.encode(user_query))\n",
    "output_tokens = 8192 # assuming full response\n",
    "estimated_cost = estimate_cost(input_tokens, output_tokens, pricing_df)\n",
    "\n",
    "print(f\"\\nEstimated cost: ${estimated_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the cost per message above, and add additional file/folder filters if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This is a test error to prevent automatically querying model and incurring costs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# raise error to prevent automatically querying model and incurring costs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a test error to prevent automatically querying model and incurring costs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: This is a test error to prevent automatically querying model and incurring costs"
     ]
    }
   ],
   "source": [
    "# raise error to prevent automatically querying model and incurring costs\n",
    "raise ValueError(\"This is a test error to prevent automatically querying model and incurring costs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's response:\n",
      "Sure, here's a table for each class file in the ras_commander library, with all functions, their arguments (with typing/expected input), and a short summary of the function's purpose.\n",
      "\n",
      "1. `RasCommander.py`\n",
      "\n",
      "| Function | Arguments | Summary |\n",
      "|----------|-----------|---------|\n",
      "| `compute_plan` | `plan_number: str`, `compute_folder: Optional[Union[str, Path]] = None`, `ras_object: Optional[RasPrj] = None` | Execute a HEC-RAS plan. |\n",
      "| `compute_test_mode` | `plan_numbers: Optional[List[str]] = None`, `folder_suffix: str = '[Test]'`, `clear_geompre: bool = False`, `max_cores: Optional[int] = None`, `ras_object: Optional[RasPrj] = None` | Execute HEC-RAS plans in test mode. |\n",
      "| `compute_parallel` | `plan_numbers: Optional[List[str]] = None`, `max_workers: int = 2`, `cores_per_run: int = 2`, `ras_object: Optional[RasPrj] = None`, `dest_folder: Optional[Union[str, Path]] = None` | Execute HEC-RAS plans in parallel using multiple worker threads. |\n",
      "| `worker_thread` | `worker_id: int` | A worker thread function for parallel execution. |\n",
      "\n",
      "2. `RasGeo.py`\n",
      "\n",
      "| Function | Arguments | Summary |\n",
      "|----------|-----------|---------|\n",
      "| `clear_geompre_files` | `plan_files: Optional[Union[str, Path, List[Union[str, Path]]]] = None`, `ras_object: Optional[RasPrj] = None` | Clear HEC-RAS geometry preprocessor files. |\n",
      "\n",
      "3. `RasPlan.py`\n",
      "\n",
      "| Function | Arguments | Summary |\n",
      "|----------|-----------|---------|\n",
      "| `set_geom` | `plan_number: Union[str, int]`, `new_geom: Union[str, int]`, `ras_object: Optional[RasPrj] = None` | Set the geometry for the specified plan. |\n",
      "| `set_steady` | `plan_number: str`, `new_steady_flow_number: str`, `ras_object: Optional[RasPrj] = None` | Apply a steady flow file to a plan file. |\n",
      "| `set_unsteady` | `plan_number: str`, `new_unsteady_flow_number: str`, `ras_object: Optional[RasPrj] = None` | Apply an unsteady flow file to a plan file. |\n",
      "| `set_num_cores` | `plan_number: Union[str, Path]`, `num_cores: int`, `ras_object: Optional[RasPrj] = None` | Update the maximum number of cores to use in the HEC-RAS plan file. |\n",
      "| `set_geom_preprocessor` | `file_path: Union[str, Path]`, `run_htab: int`, `use_ib_tables: int`, `ras_object: Optional[RasPrj] = None` | Update the simulation plan file to modify the 'Run HTab' and 'UNET Use Existing IB Tables' settings. |\n",
      "| `get_results_path` | `plan_number: str`, `ras_object: Optional[RasPrj] = None` | Retrieve the results file path for a given HEC-RAS plan number. |\n",
      "| `get_plan_path` | `plan_number: str`, `ras_object: Optional[RasPrj] = None` | Return the full path for a given plan number. |\n",
      "| `get_flow_path` | `flow_number: str`, `ras_object: Optional[RasPrj] = None` | Return the full path for a given flow number. |\n",
      "| `get_unsteady_path` | `unsteady_number: str`, `ras_object: Optional[RasPrj] = None` | Return the full path for a given unsteady number. |\n",
      "| `get_geom_path` | `geom_number: str`, `ras_object: Optional[RasPrj] = None` | Return the full path for a given geometry number. |\n",
      "| `clone_plan` | `template_plan: str`, `new_plan_shortid: Optional[str] = None`, `ras_object: Optional[RasPrj] = None` | Create a new plan file based on a template and update the project file. |\n",
      "| `clone_unsteady` | `template_unsteady: str`, `ras_object: Optional[RasPrj] = None` | Copy unsteady flow files from a template, find the next unsteady number, and update the project file accordingly. |\n",
      "| `clone_steady` | `template_flow: str`, `ras_object: Optional[RasPrj] = None` | Copy steady flow files from a template, find the next flow number, and update the project file accordingly. |\n",
      "| `clone_geom` | `template_geom: str`, `ras_object: Optional[RasPrj] = None` | Copy geometry files from a template, find the next geometry number, and update the project file accordingly. |\n",
      "| `get_next_number` | `existing_numbers: List[str]` | Determine the next available number from a list of existing numbers. |\n",
      "\n",
      "4. `RasPrj.py`\n",
      "\n",
      "| Function | Arguments | Summary |\n",
      "|----------|-----------|---------|\n",
      "| `__init__` | - | Initialize a RasPrj instance. |\n",
      "| `initialize` | `project_folder: Union[str, Path]`, `ras_exe_path: Union[str, Path]` | Initialize a RasPrj instance with the given project folder and RAS executable path. |\n",
      "| `_load_project_data` | - | Load project data from the HEC-RAS project file. |\n",
      "| `_get_prj_entries` | `entry_type: str` | Extract entries of a specific type from the HEC-RAS project file. |\n",
      "| `is_initialized` | - | Check if the RasPrj instance has been initialized. |\n",
      "| `check_initialized` | - | Ensure that the RasPrj instance has been initialized. |\n",
      "| `find_ras_prj` | `folder_path: Union[str, Path]` | Find the appropriate HEC-RAS project file (.prj) in the given folder. |\n",
      "| `get_project_name` | - | Get the name of the HEC-RAS project. |\n",
      "| `get_prj_entries` | `entry_type: str` | Get entries of a specific type from the HEC-RAS project. |\n",
      "| `get_plan_entries` | - | Get all plan entries from the HEC-RAS project. |\n",
      "| `get_flow_entries` | - | Get all flow entries from the HEC-RAS project. |\n",
      "| `get_unsteady_entries` | - | Get all unsteady flow entries from the HEC-RAS project. |\n",
      "| `get_geom_entries` | - | Get all geometry entries from the HEC-RAS project. |\n",
      "| `get_hdf_entries` | - | Get HDF entries for plans that have results. |\n",
      "| `print_data` | - | Print all RAS Object data for this instance. |\n",
      "| `init_ras_project` | `ras_project_folder: str`, `ras_version: str`, `ras_instance: Optional[RasPrj] = None` | Initialize a RAS project. |\n",
      "| `get_ras_exe` | `ras_version: str` | Determine the HEC-RAS executable path based on the input. |\n",
      "\n",
      "5. `RasUnsteady.py`\n",
      "\n",
      "| Function | Arguments | Summary |\n",
      "|----------|-----------|---------|\n",
      "| `update_unsteady_parameters` | `unsteady_file: Union[str, Path]`, `modifications: Dict[str, Any]`, `ras_object: Optional[RasPrj] = None` | Modify parameters in an unsteady flow file. |\n",
      "\n",
      "6. `RasUtils.py`\n",
      "\n",
      "| Function | Arguments | Summary |\n",
      "|----------|-----------|---------|\n",
      "| `create_backup` | `file_path: Path`, `backup_suffix: str = '_backup'`, `ras_object: Optional[RasPrj] = None` | Create a backup of the specified file. |\n",
      "| `restore_from_backup` | `backup_path: Path`, `remove_backup: bool = True`, `ras_object: Optional[RasPrj] = None` | Restore a file from its backup. |\n",
      "| `create_directory` | `directory_path: Path`, `ras_object: Optional[RasPrj] = None` | Ensure that a directory exists, creating it if necessary. |\n",
      "| `find_files_by_extension` | `extension: str`, `ras_object: Optional[RasPrj] = None` | List all files in the project directory with a specific extension. |\n",
      "| `get_file_size` | `file_path: Path`, `ras_object: Optional[RasPrj] = None` | Get the size of a file in bytes. |\n",
      "| `get_file_modification_time` | `file_path: Path`, `ras_object: Optional[RasPrj] = None` | Get the last modification time of a file. |\n",
      "| `get_plan_path` | `current_plan_number_or_path: Union[str, Path]`, `ras_object: Optional[RasPrj] = None` | Get the path for a plan file with a given plan number or path. |\n",
      "| `remove_with_retry` | `path: Path`, `max_attempts: int = 5`, `initial_delay: float = 1.0`, `is_folder: bool = True`, `ras_object: Optional[RasPrj] = None` | Attempts to remove a file or folder with retry logic and exponential backoff. |\n",
      "| `update_plan_file` | `plan_number_or_path: Union[str, Path]`, `file_type: str`, `entry_number: int`, `ras_object: Optional[RasPrj] = None` | Update a plan file with a new file reference. |\n",
      "| `check_file_access` | `file_path: Union[str, Path]`, `mode: str = 'r'` | Check if a file can be accessed with the specified mode. |\n",
      "\n",
      "7. `RasExamples.py`\n",
      "\n",
      "| Function | Arguments | Summary |\n",
      "|----------|-----------|---------|\n",
      "| `__init__` | - | Initialize the RasExamples class. |\n",
      "| `get_example_projects` | `version_number: Optional[str] = None` | Get the available example projects for the specified version. |\n",
      "| `list_categories` | - | List the available categories of example projects. |\n",
      "| `list_projects` | `category: Optional[str] = None` | List the available example projects in a given category. |\n",
      "| `extract_project` | `project_names: Union[str, List[str]]` | Extract the specified example project(s) to the local file system. |\n",
      "| `clean_projects_directory` | - | Clean the directory where example projects are extracted. |\n",
      "\n",
      "Please note that these tables are based on the code provided, and the actual implementation details may vary.\n"
     ]
    }
   ],
   "source": [
    "# Stream Claude's response\n",
    "response_text = stream_response(client, full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude's response:\n",
    "Sure, I'll review the code and identify any duplicate or redundant sections that can be removed or consolidated to reduce the overall size of the library.\n",
    "\n",
    "1. `RasFolderInit.py`:\n",
    "   - The `get_plan_entries`, `get_flow_entries`, `get_unsteady_entries`, and `get_geom_entries` functions follow a similar pattern. They can be consolidated into a single function that takes a file type as an argument.\n",
    "   - The `find_ras_prj` function could potentially be merged with the `get_project_name` function, as they both operate on project file paths.\n",
    "\n",
    "2. `RasPlan.py`:\n",
    "   - The `get_plan_path`, `get_flow_path`, `get_unsteady_path`, and `get_geom_path` functions are very similar and could be combined into a single function that takes a file type and number as arguments.\n",
    "   - The `set_geom`, `set_flow`, and `set_unsteady` functions have a lot of overlapping code, particularly for updating plan files. These could be consolidated into a single function that takes the file type and number as arguments.\n",
    "\n",
    "3. `RasUtils.py`:\n",
    "   - The `create_backup` and `restore_from_backup` functions have some overlap in their logic and could potentially be combined into a single function with an additional argument to specify whether to create or restore a backup.\n",
    "   - The `remove_file` function could be merged with `remove_folder_with_retry` by adding an optional argument to handle file removal.\n",
    "\n",
    "4. `RasCommander.py`:\n",
    "   - The `compute_plan` and `compute_plan_from_folder` functions have a lot of overlapping code. These could be combined into a single function with an optional argument to specify the folder path.\n",
    "\n",
    "5. `RasPrj.py`:\n",
    "   - The `find_ras_prj` function is already present in `RasFolderInit.py`, so it could be removed from this module.\n",
    "   - The `load_project_data` function is not used anywhere else in the codebase, so it could be removed unless it has a planned future use.\n",
    "\n",
    "After consolidating these functions and removing any redundant code, the overall size of the library should be reduced. However, it's important to ensure that the functionality remains intact and that any changes are thoroughly tested.\n",
    "\n",
    "Estimated cost: $0.2435\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freshcmdr_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
