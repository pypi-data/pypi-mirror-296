{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "import data\n",
    "data.download_arxiv(save_path)\n",
    "papers_parsed_dict = data.process_arxiv(save_path, YEARS)\n",
    "# Step 2\n",
    "import preprocessing, analyze\n",
    "grouped_work, edge_df, author_df = preprocessing.preprocessing_for_embedding(save_path, '2000', 'arxiv')\n",
    "path=f'{save_path}/2000/arxiv/'\n",
    "data_object = analyze.embed_and_save(path,'full', 'tfidf', grouped_work, edge_df, author_df)\n",
    "logging.info(f\"Finished embedding for {path}\")\n",
    "analyze.calc_save_network_statistics(path,'full', data_object)\n",
    "# Step 3\n",
    "import preprocessing\n",
    "path=f'{save_path}/2000/arxiv/full'\n",
    "preprocessing.preprocessing_for_model(path,'tfidf','gat')\n",
    "# Step 4\n",
    "# ZEROSHOT\n",
    "import torch\n",
    "save_path='/ix/djishnu/Common_Folder/Coauth_Rev/finale2'\n",
    "year, data, component, embedding, model, control = '2000', 'arxiv', 'full', 'tfidf', 'zeroshot', None\n",
    "batch_size = 2048\n",
    "neg_ratio = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "path = f\"{save_path}/{year}/{data}/{component}/{embedding}/{model}\"\n",
    "if control is not None:\n",
    "    path = f\"{path}/{control}\"\n",
    "\n",
    "if 'zeroshot' not in model:\n",
    "        logging.error(\"This scripts only supports 'zeroshot'. 'zeroshot' not found in models. Hence skipping.\")\n",
    "        exit()\n",
    "\n",
    "import analyze, utils\n",
    "all_data, _, _, test_data = utils._read_data(path,device)\n",
    "analyze.run_zeroshot(path, all_data,test_data,batch_size,neg_ratio,device)\n",
    "# Step 4\n",
    "# GAT\n",
    "from torch_geometric.nn import GAT\n",
    "import torch\n",
    "class GAT_Module(torch.nn.Module):\n",
    "    def __init__(self, out_channels = 256, hidden_layers = 2, v2=True, concat=True,heads=1,add_self_loops=False):\n",
    "        super(GAT_Module,self).__init__()\n",
    "        self.gat = GAT(in_channels = -1, hidden_channels = 2*out_channels, num_layers = hidden_layers, out_channnels=out_channels, concat=concat, heads=heads, add_self_loops=add_self_loops)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x.to(torch.float32)\n",
    "        z = self.gat(x,data.edge_index)\n",
    "        return z\n",
    "\n",
    "save_path='/ix/djishnu/Common_Folder/Coauth_Rev/finale2'\n",
    "year, data, component, embedding, model, control = '2000', 'arxiv', 'full', 'tfidf', 'gat', None\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "path = f\"{save_path}/{year}/{data}/{component}/{embedding}/{model}\"\n",
    "if control is not None:\n",
    "    path = f\"{path}/{control}\"\n",
    "\n",
    "if model != 'gat' and model != 'gat_graph_embed':\n",
    "    logging.error(\"This script only supports 'gat' or 'gat_graph_embed'. 'gat' or 'gat_graph_embed' not found in models. Hence Exiting.\")\n",
    "    exit()\n",
    "\n",
    "if model == 'gat_graph_embed':\n",
    "    try:\n",
    "        assert (-1 in all_data.y) and (len(np.where((all_data.y==-1))[0])==1)\n",
    "        graph_ed=True\n",
    "        neg_node = np.where((all_data.y==-1))[0][0] # Pseudo node\n",
    "    except:\n",
    "        logging.error(f'model = {model} but -1 not found in all_data.y or more than 1 pseudo node found. Exiting...')\n",
    "        exit()\n",
    "out_channels = 256\n",
    "hidden_layers = 2\n",
    "batch_size = 2048\n",
    "neg_ratio = 1\n",
    "epochs = 4\n",
    "batch_size = 2048\n",
    "neg_ratio = 1\n",
    "log_epoch = 1\n",
    "\n",
    "gat = GAT_Module(out_channels,hidden_layers).to(device)\n",
    "optimizer = torch.optim.Adam(gat.parameters(), lr=0.01, weight_decay=0.001)\n",
    "graph_ed = False\n",
    "neg_node = None\n",
    "\n",
    "import analyze, utils\n",
    "all_data, train_data, val_data, test_data = utils._read_data(path,device)\n",
    "analyze.run_gat(path, all_data, train_data, val_data, test_data, batch_size, neg_ratio, device, gat, optimizer, epochs, log_epoch, neg_node, graph_ed)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = ['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022','2023']\n",
    "datasets = ['arxiv', 'openalex']\n",
    "graph_components = ['full','lcc']\n",
    "embedding_modes = ['tfidf','bert']\n",
    "models = ['zeroshot','gat','gat_graph_embed']\n",
    "controls = ['shuffle_y', 'shuffle_x']\n",
    "restart_level = None\n",
    "save_path='/ix/djishnu/Common_Folder/Coauth_Rev/finale3'\n",
    "save_per_year = True\n",
    "# process_restart(restart_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ix/djishnu/Swapnil/.conda/envs/coauth_env/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 34323 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dask, pandas as pd, pickle, os, logging, itertools, argparse,ast\n",
    "from dask.distributed import Client, as_completed\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from plotter import tokenize_title_per_year, process_year\n",
    "\n",
    "# Creating directories for the the analysis\n",
    "os.makedirs(f\"{save_path}/slurm_outs/5_data_combining_dask\", exist_ok=True,mode=0o775)\n",
    "if not os.path.exists(f'{save_path}/combined_year_results/tfidf_features'):\n",
    "    os.makedirs(f'{save_path}/combined_year_results/tfidf_features', exist_ok=False, mode=0o755)\n",
    "\n",
    "if not os.path.exists(f'{save_path}/combined_year_results/auc_aupr'):\n",
    "    os.makedirs(f'{save_path}/combined_year_results/auc_aupr', exist_ok=False, mode=0o755)\n",
    "\n",
    "# Setting dask client\n",
    "logging.info(\"Starting dask client...\")\n",
    "cluster = SLURMCluster(\n",
    "        cores=1,\n",
    "        memory='10GB',  # Memory per process\n",
    "        walltime='0-01:00:00',\n",
    "        account='djishnu',\n",
    "        job_extra_directives=[  '--job-name=data_combining',\n",
    "                                '--cluster=htc',\n",
    "                                f'--output={save_path}/slurm_outs/5_data_combining_dask/%x_%A_%a.out',\n",
    "                            ]\n",
    "    )\n",
    "cluster.adapt(minimum=1, maximum=128)\n",
    "client = Client(cluster)\n",
    "\n",
    "data= datasets[0]\n",
    "# country = 'ALL'\n",
    "\n",
    "# Reading the countries from the file\n",
    "countries=[]\n",
    "cntry_file_path = '/ix/djishnu/Swapnil/coauthorship/Co-Authorship/inputs/countries.txt'\n",
    "with open(f'{cntry_file_path}', 'r') as file:\n",
    "    for line in file:\n",
    "        stripped_line = line.strip()\n",
    "        if not line.startswith('#') and len(stripped_line) > 0:\n",
    "            countries.append(stripped_line)\n",
    "\n",
    "# Preparing arguments for AUC_AUPR and TFIDF\n",
    "arguments = list(itertools.product(YEARS, [data], graph_components, embedding_modes, models))\n",
    "if controls is not None:\n",
    "    arguments = arguments + list(itertools.product(YEARS, [data], graph_components, embedding_modes, models, controls))\n",
    "\n",
    "if (data =='openalex'):\n",
    "    args_for_map_auc = list(itertools.product([save_path],arguments, countries))\n",
    "    args_for_map_tfidf = list(itertools.product([save_path], YEARS, [data], countries, [save_per_year]))\n",
    "elif (data == 'arxiv'):\n",
    "    args_for_map_auc = list(itertools.product([save_path],arguments, ['ALL']))\n",
    "    args_for_map_tfidf = list(itertools.product([save_path], YEARS, [data], ['ALL'], [save_per_year]))\n",
    "else:\n",
    "    logging.error(f\"Country {country} not found in the {data}. Skipping...\")\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    exit()\n",
    "# Preparing variables for AUC_AUPR\n",
    "all_model_result, all_model_quad_result = [], []\n",
    "all_columns_labels = ['arguments','country', 'AUC','AUPR']\n",
    "quad_columns_labels = ['arguments', 'country', 'quadrant_logic', 'Q1_AuC', 'Q2_AuC', 'Q3_AuC', 'Q4_AuC', 'Q1_AuPR', 'Q2_AuPR', 'Q3_AuPR', 'Q4_AuPR']\n",
    "# Preparing variables for TFIDF\n",
    "tfidf_features = defaultdict(float)\n",
    "# Submit futures with tags and store them in a dictionary\n",
    "futures_auc = {client.submit(process_year, *args): 'auc' for args in args_for_map_auc}\n",
    "futures_tfidf = {client.submit(tokenize_title_per_year, *args): 'tfidf' for args in args_for_map_tfidf}\n",
    "\n",
    "# Combine the futures into a single dictionary\n",
    "all_futures = {**futures_auc, **futures_tfidf}\n",
    "\n",
    "# Collect results and exceptions with tags\n",
    "results_auc, results_tfidf, exceptions = [], [], []\n",
    "\n",
    "for future in as_completed(all_futures):\n",
    "    tag = all_futures[future]\n",
    "    try:\n",
    "        result = future.result()\n",
    "        if tag == 'auc':\n",
    "            results_auc.append(result)\n",
    "        else:\n",
    "            results_tfidf.append(result)\n",
    "    except Exception as e:\n",
    "        exceptions.append((future, e))\n",
    "\n",
    "#Combining auc_aupr features across years\n",
    "for i,result in enumerate(results_auc):\n",
    "    if (result[0] is not None) and (result[1] is not None) and (result[2] is not None):   \n",
    "        all_model_result.append(result[0])\n",
    "        all_model_quad_result.append(result[1])\n",
    "        all_model_quad_result.append(result[2])\n",
    "    else:\n",
    "        continue\n",
    "all_model_result = [entry if isinstance(entry, (list, tuple)) else [entry] for entry in all_model_result] #Dealing with NaNs\n",
    "all_model_quad_result = [entry if isinstance(entry, (list, tuple)) else [entry] for entry in all_model_quad_result] #Dealing with NaNs\n",
    "pd.DataFrame(all_model_result, columns=all_columns_labels).to_csv(f'{save_path}/combined_year_results/auc_aupr/{data}_compiled_results.csv',index=False)\n",
    "pd.DataFrame(all_model_quad_result, columns=quad_columns_labels).to_csv(f'{save_path}/combined_year_results/auc_aupr/{data}_compiled_quad_results.csv',index=False)\n",
    "logging.info(f\"Completed AUC AUPR for {data}\")\n",
    "\n",
    "#Combining tfidf features across years\n",
    "for result in results_tfidf:\n",
    "    if result is not None:\n",
    "        for key, value in result.items():\n",
    "            tfidf_features[key] += value\n",
    "pickle.dump(tfidf_features, open(f'{save_path}/combined_year_results/tfidf_features/{data}_tfidf_combined_across_years.pkl', 'wb'))\n",
    "logging.info(f\"Completed embedding for {data}\")\n",
    "# Shut down the cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= datasets[0]\n",
    "if not os.path.exists(f'{save_path}/combined_year_results/tfidf_features'):\n",
    "    os.makedirs(f'{save_path}/combined_year_results/tfidf_features', exist_ok=False, mode=0o755)\n",
    "\n",
    "if not os.path.exists(f'{save_path}/combined_year_results/auc_aupr'):\n",
    "    os.makedirs(f'{save_path}/combined_year_results/auc_aupr', exist_ok=False, mode=0o755)\n",
    "arguments = list(itertools.product(YEARS, [data], graph_components, embedding_modes, models))\n",
    "countries = ['ALL']\n",
    "for country in countries:\n",
    "        if (data =='openalex') or (data == 'arxiv' and country == 'ALL'):\n",
    "            try:\n",
    "                calc_auc_aupr_across_years(save_path, data, arguments, country)\n",
    "                print(f\"Finished calculating AUC and AUPR for country {country} in {data} data.\", flush=True)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in calculating AUC and AUPR for country {country} in {data} data. Error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "            #     results = pool.starmap(tokenize_title_per_year, [(save_path, year, data, country,save_per_year) for year in YEARS])\n",
    "            \n",
    "            # #Combining tfidf features across years\n",
    "            # tfidf_features = defaultdict(float) \n",
    "            # for result in results:\n",
    "            #     for key, value in result.items():\n",
    "            #         tfidf_features[key] += value\n",
    "            # pickle.dump(tfidf_features, open(f'{save_path}/combined_year_results/tfidf_features/{country}_{data}_tfidf_combined_across_years.pkl', 'wb'))\n",
    "        else:\n",
    "            logging.error(f\"Country {country} not found in the {data}. Skipping...\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotter, preprocessing,pickle,numpy as np\n",
    "from utils import _LemmaTokenizer, _HexCodeFromFrequencyDict\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "for dataset in datasets:\n",
    "    for year in YEARS:\n",
    "        path=f'{save_path}/{year}/{dataset}/'\n",
    "        tfidf_features = defaultdict(float)\n",
    "        def tokenize_title_across_years(paper_titles,tfidf_features):\n",
    "            lemma_tokenizer=_LemmaTokenizer()\n",
    "            stop_words = set(stopwords.words('english')) \n",
    "            token_stop = lemma_tokenizer(' '.join(stop_words))\n",
    "            pipe = Pipeline([('count', CountVectorizer(tokenizer=lemma_tokenizer, stop_words=token_stop, max_features=768)), ('tfid', TfidfTransformer())])\n",
    "            pipe.fit(paper_titles)\n",
    "            cts = pipe['count'].transform(paper_titles)\n",
    "            for key, value in zip(pipe.get_feature_names_out(), np.sum(cts,axis=0).A1 / (1.0*len(paper_titles))):\n",
    "                tfidf_features[key]+=(value)\n",
    "            return tfidf_features\n",
    "\n",
    "        with open(f'{path}/paper_titles.pkl', 'rb') as f:\n",
    "            paper_titles = pickle.load(f)\n",
    "        tfidf_features = tokenize_title_across_years(paper_titles, tfidf_features)\n",
    "    \n",
    "    wc = wordcloud.WordCloud(background_color='white',\n",
    "                         color_func=_HexCodeFromFrequencyDict(tfidf_features), \n",
    "                         height=400)\n",
    "    wc.generate_from_frequencies(tfidf_features)\n",
    "    with open(f'{save_path}/combined_files/{dataset}_wc_all_yrs.svg',\"w\") as f:\n",
    "        f.write(wc.to_svg(embed_font=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
