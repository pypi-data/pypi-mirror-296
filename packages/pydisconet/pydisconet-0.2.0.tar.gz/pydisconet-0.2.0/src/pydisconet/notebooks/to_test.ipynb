{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import _default_inputs, _display_input_help\n",
    "import os, itertools, shutil, subprocess, ast, yaml, logging,glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_restart(recreate_folder_level=None, save_path=None, YEARS=None, datasets=None, graph_components=None, embedding_modes=None, models=None, controls=None):\n",
    "    if recreate_folder_level is not None:\n",
    "        recreate_folder_level = recreate_folder_level[0]\n",
    "        print(f\"Restart values found as: {recreate_folder_level}. Recreating the paths for the restart values\")\n",
    "        if recreate_folder_level == 'years':\n",
    "            paths = itertools.product(YEARS)\n",
    "        elif recreate_folder_level == 'datasets':\n",
    "            paths = itertools.product(YEARS, datasets)\n",
    "        elif recreate_folder_level == 'graph_components':\n",
    "            paths = itertools.product(YEARS, datasets, graph_components)\n",
    "        elif recreate_folder_level == 'embedding_modes':\n",
    "            paths = itertools.product(YEARS, datasets, graph_components, embedding_modes)\n",
    "        elif recreate_folder_level == 'models':\n",
    "            paths = itertools.product(YEARS, datasets, graph_components, embedding_modes, models)\n",
    "        elif recreate_folder_level == 'controls':\n",
    "            paths = itertools.product(YEARS, datasets, graph_components, embedding_modes, models, controls)\n",
    "        else:\n",
    "            raise ValueError(f\"Restart level: {recreate_folder_level} not recognized. Please use 'years', 'datasets', 'graph_components', 'embedding_modes', 'models', 'controls'\")\n",
    "        \n",
    "        for comb in paths:\n",
    "            path = os.path.join(save_path, *comb)\n",
    "            if os.path.exists(path):\n",
    "                shutil.rmtree(path)\n",
    "            print(path)\n",
    "           \n",
    "def check_list_inputs(level, actual_input, allowed_input):\n",
    "    if actual_input is None:\n",
    "        if level not in ['controls', 'restart_level']:\n",
    "            raise ValueError(f\"Input is None. Please use ATLEAST ONE of the following: {allowed_input}\")\n",
    "        else :\n",
    "            logging.info(f\"{level} is None\")\n",
    "    else: \n",
    "        for item in actual_input:\n",
    "            if item not in allowed_input:\n",
    "                raise ValueError(f\"Input {item} not recognized. Please use ATLEAST ONE of the following: {allowed_input}\") \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputs(user_inputs):\n",
    "    process_input = _default_inputs()\n",
    "    for key, value in user_inputs.items():\n",
    "        try:\n",
    "            process_input[key] = value\n",
    "        except KeyError:\n",
    "            logging.error(f\"Key {key} is not a valid input. Ignoring it and continuing...\")\n",
    "            _display_input_help()\n",
    "        \n",
    "    consec_yrs_gat = process_input['consec_yrs_gat']\n",
    "    consec_yrs_auth = process_input['consec_yrs_auth']\n",
    "    start_year = process_input['start_year']\n",
    "    end_year = process_input['end_year']\n",
    "    save_path = process_input['save_path']\n",
    "\n",
    "\n",
    "    check_list_inputs('datasets',process_input['datasets'], ['openalex','arxiv'])\n",
    "    check_list_inputs('graph_components',process_input['graph_components'], ['full','lcc'])\n",
    "    check_list_inputs('embedding_modes',process_input['embedding_modes'], ['tfidf','bert'])\n",
    "    check_list_inputs('models',process_input['models'], ['zeroshot','gat','gat_graph_embed'])\n",
    "    check_list_inputs('controls',process_input['controls'], ['shuffle_y','shuffle_x', None])\n",
    "    check_list_inputs('recreate_folder_level',process_input['recreate_folder_level'], ['years', 'datasets', 'graph_components', 'embedding_modes', 'models', 'controls', None])\n",
    "    \n",
    "    \n",
    "    if (consec_yrs_gat is not None and (consec_yrs_gat > end_year - start_year + 1)) or (consec_yrs_auth is not None and (consec_yrs_auth > end_year - start_year + 1)):\n",
    "        raise ValueError(\"Number of consecutive years for GAT/ common authors cannot be more than the total number of years.\")\n",
    "\n",
    "    YEARS = [str(year) for year in range(start_year, end_year+1)]\n",
    "    if (consec_yrs_gat is not None and consec_yrs_gat > 1) :\n",
    "        logging.info(f\"Consecutive years for GAT is set to {consec_yrs_gat}. This will create all possible gat_X_X+{consec_yrs_gat-1} years\")\n",
    "        year_consec_gat = [f'gat_{year}_{year+consec_yrs_gat-1}' for year in range(start_year, end_year + 2 - consec_yrs_gat)]\n",
    "        YEARS = YEARS + year_consec_gat\n",
    "\n",
    "    if (consec_yrs_auth is not None and consec_yrs_auth > 1) :\n",
    "        logging.info(f\"Consecutive years for common authors is set to {consec_yrs_auth}. This will create all possible auth_X_X+{consec_yrs_auth-1} years\")\n",
    "        year_consec_auth = [f'auth_{year}_{year+consec_yrs_auth-1}' for year in range(start_year, end_year + 2 - consec_yrs_auth)]\n",
    "        YEARS = YEARS + year_consec_auth\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path, exist_ok=False, mode=0o755)\n",
    "        \n",
    "    return process_input, YEARS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shell_script_cpu(save_path, commands,job_name):\n",
    "    sbatch_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --cluster=htc\n",
    "#SBATCH --array=1-{str(len(commands))}\n",
    "#SBATCH -t 0-06:00:00\n",
    "#SBATCH --output={save_path}/slurm_outs/{job_name}/%x_%A_%a.out\n",
    "#SBATCH --ntasks=16\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem-per-cpu=10G\n",
    "##SBATCH --mail-user=swk25@pitt.edu\n",
    "##SBATCH --mail-type=END,FAIL\n",
    "\n",
    "# module purge\n",
    "# module load gcc/8.2.0\n",
    "# module load python/anaconda3.10-2022.10\n",
    "# source activate /ix/djishnu/Swapnil/.conda/envs/coauth_env/\n",
    "\n",
    "\"\"\"\n",
    "    return sbatch_script\n",
    "\n",
    "def shell_script_gpu(save_path, commands,job_name):\n",
    "    sbatch_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --cluster=gpu\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task={1 if '3_' in job_name else 4}\n",
    "#SBATCH --mem-per-cpu=40G ### Will allocate 40*4 to each gpu ## To preven cuda oom\n",
    "#SBATCH --partition=l40s,a100,gtx1080,a100_nvlink\n",
    "#SBATCH --array=1-{str(len(commands))}\n",
    "#SBATCH -t 0-06:00:00\n",
    "#SBATCH --output={save_path}/slurm_outs/{job_name}/%x_%A_%a.out\n",
    "##SBATCH --mail-user=swk25@pitt.edu\n",
    "##SBATCH --mail-type=END,FAIL\n",
    "\n",
    "# module purge\n",
    "# module load gcc/8.2.0\n",
    "# module load python/anaconda3.10-2022.10\n",
    "# source activate /ix/djishnu/Swapnil/.conda/envs/coauth_env/\n",
    "\n",
    "\"\"\"\n",
    "    return sbatch_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     user_inputs \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(file)\n\u001b[1;32m      4\u001b[0m FINAL_INPUTS, YEARS \u001b[38;5;241m=\u001b[39m process_inputs(user_inputs)\n",
      "File \u001b[0;32m/ix/djishnu/Swapnil/.conda/envs/coauth_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input.yaml'"
     ]
    }
   ],
   "source": [
    "with open('input.yaml', 'r') as file:\n",
    "    user_inputs = yaml.safe_load(file)\n",
    "\n",
    "FINAL_INPUTS, YEARS = process_inputs(user_inputs)\n",
    "read_path = FINAL_INPUTS['read_path']\n",
    "save_path =FINAL_INPUTS['save_path']\n",
    "years = YEARS\n",
    "datasets = FINAL_INPUTS['datasets']\n",
    "graph_components = FINAL_INPUTS['graph_components']\n",
    "embedding_modes = FINAL_INPUTS['embedding_modes']\n",
    "models = FINAL_INPUTS['models']\n",
    "controls = FINAL_INPUTS['controls']\n",
    "\n",
    "all_combinations = list(itertools.product(years, datasets, graph_components, embedding_modes, models, controls))\n",
    "if FINAL_INPUTS['recreate_folder_level'][0] is not None:\n",
    "        process_restart(FINAL_INPUTS['recreate_folder_level'], save_path,years, datasets, graph_components, embedding_modes, models, controls)\n",
    "        for comb in all_combinations:\n",
    "            path = os.path.join(save_path, *comb)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path, exist_ok=False, mode=0o755)\n",
    "\n",
    "def develop_chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "    return lst\n",
    "\n",
    "def step1(arguments, job_name):\n",
    "    if len(arguments) >500:\n",
    "        chunks = develop_chunks(arguments, 500)\n",
    "    else:\n",
    "        chunks = [arguments]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if os.path.exists(f'{save_path}/slurm_scripts/{job_name}_{i}.sh'):\n",
    "            os.remove(f'{save_path}/slurm_scripts/{job_name}_{i}.sh')\n",
    "        os.makedirs(f'{save_path}/slurm_outs/{job_name}_{i}', exist_ok=True, mode=0o755)\n",
    "        with open(f'{save_path}/slurm_scripts/{job_name}_{i}.sh', \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(shell_script_cpu(save_path, chunk, job_name = f\"{job_name}_{i}\" ))\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"commands=(\\n\")\n",
    "            for combo in chunk:\n",
    "                f.write(f\"\\t \\\"python 1_data_downloading.py --parameters \\\"\\\\\\\"\\\"({YEARS},'{combo}')\\\"\\\\\\\"\\\" --read_path '{read_path}' --save_path '{save_path}' \\\" \\n \")\n",
    "            f.write(\")\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"eval ${commands[$SLURM_ARRAY_TASK_ID-1]}\\n\")\n",
    "            f.write(\"\\ncrc-job-stats\\n\")\n",
    "    return\n",
    "\n",
    "def step2(arguments, job_name):\n",
    "    if len(arguments) >500:\n",
    "        chunks = develop_chunks(arguments, 500)\n",
    "    else:\n",
    "        chunks = [arguments]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if os.path.exists(f'{save_path}/slurm_scripts/{job_name}_{i}.sh'):\n",
    "            os.remove(f'{save_path}/slurm_scripts/{job_name}_{i}.sh')\n",
    "        os.makedirs(f'{save_path}/slurm_outs/{job_name}_{i}', exist_ok=True, mode=0o755)\n",
    "        with open(f'{save_path}/slurm_scripts/{job_name}_{i}.sh', \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(shell_script_gpu(save_path, chunk, job_name = f\"{job_name}_{i}\" ))\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"commands=(\\n\")\n",
    "            for combo in chunk:\n",
    "                f.write(f\"\\t \\\"python 2_embedding_datasets.py --parameters \\\"\\\\\\\"\\\"{combo}\\\"\\\\\\\"\\\" --save_path '{save_path}' \\\" \\n \")\n",
    "            f.write(\")\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"eval ${commands[$SLURM_ARRAY_TASK_ID-1]}\\n\")\n",
    "            f.write(\"\\ncrc-job-stats\\n\")\n",
    "    return\n",
    "\n",
    "def step3(arguments, job_name):\n",
    "    if len(arguments) >500:\n",
    "        chunks = develop_chunks(arguments, 500)\n",
    "    else:\n",
    "        chunks = [arguments]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if os.path.exists(f'{save_path}/slurm_scripts/{job_name}_{i}.sh'):\n",
    "            os.remove(f'{save_path}/slurm_scripts/{job_name}_{i}.sh')\n",
    "        os.makedirs(f'{save_path}/slurm_outs/{job_name}_{i}', exist_ok=True, mode=0o755)\n",
    "        with open(f'{save_path}/slurm_scripts/{job_name}_{i}.sh', \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(shell_script_gpu(save_path, chunk, job_name = f\"{job_name}_{i}\" ))\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"commands=(\\n\")\n",
    "            for combo in chunk:\n",
    "                f.write(f\"\\t \\\"python 3_preparing_objects_network.py --parameters \\\"\\\\\\\"\\\"{combo}\\\"\\\\\\\"\\\" --save_path '{save_path}' \\\" \\n \")\n",
    "            f.write(\")\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"eval ${commands[$SLURM_ARRAY_TASK_ID-1]}\\n\")\n",
    "            f.write(\"\\ncrc-job-stats\\n\")\n",
    "    return\n",
    "\n",
    "def step4(arguments, job_name):\n",
    "    if len(arguments) >500:\n",
    "        chunks = develop_chunks(arguments, 500)\n",
    "    else:\n",
    "        chunks = [arguments]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if os.path.exists(f'{save_path}/slurm_scripts/{job_name}_{i}.sh'):\n",
    "            os.remove(f'{save_path}/slurm_scripts/{job_name}_{i}.sh')\n",
    "        os.makedirs(f'{save_path}/slurm_outs/{job_name}_{i}', exist_ok=True, mode=0o755)\n",
    "        with open(f'{save_path}/slurm_scripts/{job_name}_{i}.sh', \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(shell_script_gpu(save_path, chunk, job_name = f\"{job_name}_{i}\" ))\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"commands=(\\n\")\n",
    "            for combo in chunk:\n",
    "                if 'zeroshot' in combo:\n",
    "                    f.write(f\"\\t \\\"python 4.1_zeroshot.py --parameters \\\"\\\\\\\"\\\"{combo}\\\"\\\\\\\"\\\" --save_path '{save_path}' \\\" \\n \")\n",
    "                elif 'gat' in combo or 'gat_graph_embed' in combo:\n",
    "                    f.write(f\"\\t \\\"python 4.2_gat_gat_embed.py --parameters \\\"\\\\\\\"\\\"{combo}\\\"\\\\\\\"\\\" --save_path '{save_path}' \\\" \\n \")\n",
    "            f.write(\")\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"eval ${commands[$SLURM_ARRAY_TASK_ID-1]}\\n\")\n",
    "            f.write(\"\\ncrc-job-stats\\n\")\n",
    "    return\n",
    "\n",
    "# def step5(arguments, job_name):\n",
    "#     job_name = '5.1_openalex_full'\n",
    "#     os.makedirs(f'{save_path}/slurm_outs/{job_name}', exist_ok=True, mode=0o755)\n",
    "#     for combo in arguments:\n",
    "#         command = f\"\"\"\\t \"python 5.1_openalex_full.py --parameters \"{combo}\" \\n\" \"\"\"\n",
    "#         script_name = '_'.join(map(str, combo)) + '.sh'\n",
    "#         with open(script_name, \"w\") as f:\n",
    "#             f.write(shell_script_gpu(save_path, command, job_name = job_name))\n",
    "\n",
    "#     subprocess.run([\"sbatch\", script_name])\n",
    "#\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_job(step):\n",
    "    os.makedirs(f'{save_path}/slurm_scripts/', exist_ok=True, mode=0o755)\n",
    "    if step == 1:\n",
    "        job_name = '1_data_downloading'\n",
    "        arguments = FINAL_INPUTS['datasets']\n",
    "        step1(arguments,job_name)\n",
    "    elif step == 2:\n",
    "        job_name = '2_embedding_datasets'\n",
    "        arguments = list(itertools.product(YEARS, datasets, graph_components, embedding_modes,[None], [None]))\n",
    "        step2(arguments,job_name)\n",
    "    elif step == 3:\n",
    "        job_name = '3_preparing_objects_network'\n",
    "        arguments = list(itertools.product(YEARS, datasets, graph_components, embedding_modes, models, [None]))\n",
    "        arguments = arguments + list(itertools.product(YEARS, datasets, graph_components, embedding_modes, models, controls))\n",
    "        step3(arguments,job_name)\n",
    "    elif step == 4:\n",
    "        job_name = '4_running_models'\n",
    "        arguments = list(itertools.product(YEARS, datasets, graph_components, embedding_modes, models, [None]))\n",
    "        arguments = arguments + list(itertools.product(YEARS, datasets, graph_components, embedding_modes, models, controls))\n",
    "        step4(arguments,job_name)\n",
    "    # elif step == 5:\n",
    "    #     job_name = '5_compiling_results'\n",
    "    #     arguments = list(itertools.product(YEARS, datasets, graph_components, embedding_modes, models, [None]))\n",
    "    #     step5(arguments,job_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Step {step} not recognized. Please use 1,2,3,4\")\n",
    "    \n",
    "    script_files = glob.glob(f'{save_path}/slurm_scripts/{job_name}*.sh')\n",
    "    for script_file in script_files:\n",
    "        subprocess.run([\"sbatch\", script_file])\n",
    "        logging.info(f\"Submitted {script_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 884433 on cluster gpu\n"
     ]
    }
   ],
   "source": [
    "run_job(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib, json, duckdb, logging, shutil, multiprocessing as mp\n",
    "import urllib.request\n",
    "\n",
    "def get_data(url, record_count, save_path):\n",
    "    s3_path = url\n",
    "    save_name = url.split('/')[-2] + '_' + url.split('/')[-1].split('.')[0]\n",
    "    if not os.path.exists(f\"{save_path}/openalex_raw_data/{save_name}.json\"):\n",
    "        logging.info(f\"Downloading data from {url} with {record_count} records...\")\n",
    "        conn = duckdb.connect()\n",
    "        conn.execute(\"INSTALL aws; INSTALL httpfs; LOAD aws; LOAD httpfs; LOAD 's3';\")\n",
    "        conn.execute(\"CREATE SECRET (TYPE S3, PROVIDER CREDENTIAL_CHAIN);\")\n",
    "        conn.execute(\"SET threads TO 4; SET enable_progress_bar = false;\") ## To avoid oom error\n",
    "        conn.execute(\"SET temp_directory = '{save_path}/temp';\")\n",
    "        \n",
    "        # order FROM(identifying the data source) -> WHERE clause (applying filters) ->SELECT clause (choosing which columns to display). \n",
    "        # cannot select columns based on the results of a filter\n",
    "        # because the columns to be returned are determined before the WHERE clause filters are applied.\n",
    "        query= f\"\"\"\n",
    "                COPY (\n",
    "                SELECT  id,\n",
    "                        display_name,\n",
    "                        publication_year,\n",
    "                        publication_date,\n",
    "                        primary_location,\n",
    "                        open_access,\n",
    "                        indexed_in,\n",
    "                        institutions_distinct_count,\n",
    "                        authorships\n",
    "                FROM read_json('{s3_path}')\n",
    "                WHERE   type = 'article' AND\n",
    "                        is_paratext = false AND\n",
    "                        is_retracted = false AND\n",
    "                        publication_year >=2000 AND\n",
    "                        publication_year <=2023\n",
    "                ) TO \n",
    "                '{save_path}/openalex_raw_data/{save_name}.json' (FORMAT JSON);\n",
    "                \"\"\"\n",
    "        papers = 0\n",
    "        if record_count > 0:\n",
    "            try:\n",
    "                query_result = conn.execute(query).fetchall()\n",
    "                papers = query_result[0][0]\n",
    "            except:\n",
    "                # query returned no result. Might be because filtered out based on filters or some error\n",
    "                papers = -1\n",
    "                logging.error(f\"Query returned no result for {url}\")\n",
    "        else:\n",
    "            papers = record_count\n",
    "        logging.info(f\"Downloaded {papers} papers from {url}\")\n",
    "        conn.close()\n",
    "        return (url, papers)\n",
    "    else:\n",
    "        logging.warning(f\"Data already exists for {url}. Skipping download...\")\n",
    "        return (url, -1)\n",
    "\n",
    "def download_openalex(save_path, force_download=False):\n",
    "    try:\n",
    "        if os.path.exists(f'{save_path}, openalex_raw_data') and force_download == True:\n",
    "            logging.warning(f\"Data already exists in {save_path}/openalex_raw_data. force_download is set to True. Deleting existing data... \")\n",
    "            shutil.rmtree(f\"{save_path}/openalex_raw_data\")\n",
    "            logging.info(f\"Deleted existing data in {save_path}/openalex_raw_data. Downloading fresh data...\")\n",
    "            os.makedirs(f\"{save_path}/openalex_raw_data\", exist_ok=True)\n",
    "            urllib.request.urlretrieve(\"https://openalex.s3.amazonaws.com/data/works/manifest\", f\"{save_path}/openalex_raw_data/manifest\")\n",
    "            with open(f\"{save_path}/openalex_raw_data/manifest\", 'r') as file:\n",
    "                manifest_data = json.load(file)\n",
    "            url_count_list = [(entry['url'], entry['meta']['record_count'],save_path) for entry in manifest_data['entries']]\n",
    "\n",
    "            with mp.Pool(processes=16) as pool:\n",
    "                results = pool.starmap(get_data, url_count_list)\n",
    "            logging.info(\"Data download step completed. Creating metadata...\")\n",
    "            urls_parsed_dict = {result[0]: result[1] for result in results}\n",
    "        elif os.path.exists(f'{save_path}/openalex_raw_data') and force_download == False:\n",
    "            urls_parsed_dict = {'NA': -1}\n",
    "            logging.warning(f\"Data already exists in {save_path}/openalex_raw_data. force_download is set to False. Skipping download...\")\n",
    "        else:\n",
    "            logging.info(f\"Downloading data from OpenAlex...\")\n",
    "            os.makedirs(f\"{save_path}/openalex_raw_data\", exist_ok=True)\n",
    "            urllib.request.urlretrieve(\"https://openalex.s3.amazonaws.com/data/works/manifest\", f\"{save_path}/openalex_raw_data/manifest\")\n",
    "            with open(f\"{save_path}/openalex_raw_data/manifest\", 'r') as file:\n",
    "                manifest_data = json.load(file)\n",
    "            url_count_list = [(entry['url'], entry['meta']['record_count'],save_path) for entry in manifest_data['entries']]\n",
    "\n",
    "            with mp.Pool(processes=16) as pool:\n",
    "                results = pool.starmap(get_data, url_count_list)\n",
    "            logging.info(\"Data download step completed. Creating metadata...\")\n",
    "            urls_parsed_dict = {result[0]: result[1] for result in results}\n",
    "    except Exception as e:\n",
    "        urls_parsed_dict = {'NA': -1}\n",
    "        logging.error(f'Error: {e}')\n",
    "    return urls_parsed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# def process_openalex(read_path, save_path, years_list):\u001b[39;00m\n\u001b[1;32m     72\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting dask client...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msave_path\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/slurm_outs/1_data_processing_dask\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o775\u001b[39m)\n\u001b[1;32m     74\u001b[0m cluster \u001b[38;5;241m=\u001b[39m SLURMCluster(\n\u001b[1;32m     75\u001b[0m         cores\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     76\u001b[0m         memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10GB\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Memory per process\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m                             ]\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     84\u001b[0m cluster\u001b[38;5;241m.\u001b[39madapt(minimum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, maximum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m65\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_path' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
