# Data

model_path: meta-llama/Meta-Llama-3.1-8B

## Extract/Transform
dataset_name: HuggingFaceFW/fineweb-edu
dataset_remote_name: sample-10BT
dataset_split: train
shard_size: 100000000
data_dir: edu_fineweb10B

# Training

## Seed
seed: 42

## Wandb
project: modeldemo
run_name: llama3.1-8b-fineweb10B
log_freq: 1

## Data Load
total_bs: 65536  # 2**19, ~0.5M, in number of tokens for total 10B tokens
bs: 4 # micro batch size, 4 = RTX 3090 max
seq_len: 1024

## Model
load_in_4bit: true
load_in_8bit: false

## Lora (only when loading in 4/8 bit)
rank: 16
alpha: 32
dropout: 0.0
use_rslora: true
init_lora_weights: "gaussian"

## Optimizer (AdamW)
max_lr: !!float 6e-4
beta1: 0.9
beta2: 0.95
eps: !!float 1e-8
wd: 0.1

## LR Scheduler (OneCycle)
max_steps: 2  # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens
pct_start: 0.3
anneal_strategy: "cos" # options = 'cos', 'linear'
cycle_momentum: True
base_momentum: 0.85
max_momentum: 0.95
div_factor: 25.0
final_div_factor: 10000.0
three_phase: False

## Loop
eval_interval: 1
val_steps: 1
log_ckpt: True
num_return_sequences: 4
max_length: 32
text: "Hello, I'm a language model,"